# Install packages: 
# If you are installing from the CRAN repository, use the Install Packages function with the name of the package you want 
# to install in quotes between the parentheses. Note, you can use either single or double quotes. For example, if you want 
# to install the package ggplot2, you would use install.packages("ggplot2"). 
# If you want to install multiple packages at once, you can do so by using a character vector with the names of the packages 
# separated by commas as formatted here install.packages(c("ggplot2", "devtools", "lme4")).

# Install Bioconductor Packages:
# To install core packages, type the following in an R command window:
if (!requireNamespace("BiocManager", quietly = TRUE))
        install.packages("BiocManager")
BiocManager::install()

# Install specific packages, e.g., “GenomicFeatures” and “AnnotationDbi”, with:
BiocManager::install(c("GenomicFeatures", "AnnotationDbi"))

# Installing packages with GitHub:
install.packages("devtools")
library(devtools)
install_github("author/package")

# There is an order to loading packages, some packages require other packages 
# to be loaded first, aka dependencies. That package manual/help page
# will help you out in finding that order. 

# What packages are installed?
installed.packages()
library()

# Updating packages:
old.packages()# to check which packages need an update since you install/last updated them
update.packages()# to update all packages
install.packages("packagename")#if you want to update specific packages
# Be careful, some times an update can change the functionality of certain functions 
# so on rerunning some old code, the command might be changed or perhaps even outright gone and you need to update your code

# To unload a package in the middle of a script, the package you have loaded may not play nicely with the package you want to use
detach()
detach("package:ggplot2", unload = TRUE)# or unload a package by simply uncheck the box

# To remove a package (if you no longer want a package installed)
remove.packages()
remove.packages("ggplot")# or click on the x in the packages 

# To check the R vresion 
version
sessionInfo()# it will tell you wat version of R you are running along with all the packages you are running

# To see the functions that come with a package
help(package = "ggplot2")# or click on the package name in packages and then click on the function names for further info
browseVignettes()# these are extended help file that include an overveiew of the package and functions and detailed examples on 
# how to use the functions in plain words that you can follow along to see how you can use the package 
browseVignettes("ggplot2")

# Version control
# Version control is a system that records changes that are made to your file or set of files over time. As you make edits,
# the version control system takes snapshots of your files and the changes and then saves those snapshots so you can refer,
# revert back to previous versions later if need be. Version control systems like Git are capable of meticulously tracking sucessive 
# changes on many files with potentially many people working simultaneously on the same group of files. 
# Version control keep a record of all changes made to the files. The version control software keeps track of who, when and why 
# those specific changes were made. It's like track changes to the extreme. 
# This record is also helpful when developing code. If you realize after sometime that you made a mistake and introduced an error 
# you can find the last time you edited the particular bit of code, see the changes you made and revert back to that original, unbroken
# code leaving everything else you've done in the meanwhile untouched. 
# Version control allows multiple people to work on the same file and then helps merge all of the version of the file and all
# of their edits into one cohesive file. 
# Git is a free and open source version control system. 
# Git keeps a local copy of your work and revisions. Then once you return to internet service, you can sync your copy of the 
# work with all of your new edits and track changes to the main repository online. Additionally, since all collaborators on a project
# had their own local copy of the code, everybody can simultaneously work on their own parts of the code without disturbing the 
# common repository. 
# Git is software used locally on your computer to record changes. Github is a host for your files and the records of the 
# changes made. You can think of it as being similar to Dropbox. The files are on your computer but they are also hosted online and 
# are accessible from many computers. GitHub has the added benefit of interfacing with Git to keep track of all of your file 
# versions and changes. 
# There is a lot of vocabulary involved in working with Git
# Repository (repo): the projects folder or directory. All of your version controlled files and the recorded changes are located in it. 
# Repositories are what are hosted on GitHub and through this interface you can either keep your repositories private and share
# them with select collaborators or you can make them public. 

# Commit : to save your edits and the changes made. A commit is like a snapshot of your files. Git compares the previous version of 
# all your files in the repo to the current version and identfies those that have changed since then. For those that have not changed, 
# it maintains the previously stored file untouched. Those that have changed, it compares the files, loads the changes
# and uploads the new version of the file. 
# When you commit a file, typically you accompany that file change with a little note about what you changed and why. 
# If you find a mistake, you will revert your files to a previous commit. If you want to see what has changed in a file over time, you compare commits and look at the messages
# to see why and who. 

# Push : to update the repository with your edits. Since Git involves making changes locally, you need to be able to share 
# your changes with the common online repository. Push is sending those committed changes to that repository so now everybody
# has access to your edits. 

# Pull : to update your locall version of the repository to the current online version since others may have edited in the meanshile. 

# Staging : is the act of preparing a file for a commit. For example, if since your last commit you have edited three files for 
# completely different reasons, you dont want to commit all of the changes in one go, your message on why you are making the commit 
# in what has changed will be complicated since three files have been changed for different reasons. So instead, you can stage 
# just one of the files and prepare it for commiting. Once you've committed that file, you can stage the second file and commit it...
# So staging allows you to separate out file changes into separate commits. 

# So to summarize files are hosted in a repository that is shared with collaborators. You pull the repository's contents so 
# that you have a local copy of the files that you can edit. Once you are happy with your changes to a file, you stage
# the file and then commit it. You push this commit to the shared repository. This uploads your new file and all of the changes
# and is accompanied by a message explaining what changed, why and by whom. 

# Branch : when you are working locally in editing a file, you have created a branch where your edits are not shared with the main
# repository yet. So there are two versions of the file. The version that everybody has access to on the respository and your 
# local edited version of the file. Until you push changes and merge them back into the main repository, you are working on a branch. 

# A conflict is when multiple people make changes to the same file and Git is unable to merge the edits. Git recognized this conflict
# and asks for user assistance. You are presented with the option to maually try and merge the edits or to keep one edit over the other.

# Clone: when you make a copy of an existing Git repository. 

# A fork is a personal copy of a repository that you have taken from another person. If somebody is working on a cool project
# and you want to play around with it, you can fork their repository and then when you make changes the edits ae logged on your 
# repository not theirs. 

# Good habits to keep:
# Make purposful, single issue commits 
# Write informative messages on each commit 
# Pull and push often 


# To configure Git with Github, we need to tell Git what your username and password are:
# git config --global user.name "Maryam Ghaedi" 
# This is the name each commit will be tagged with 
# git config --global user.email maryam.ghaedi@alumni.ubc.ca
# (make sure to use the email that is linked with Github)
# To check:
# git config --list
# If you want to change these values, just retype the original config commands with your desired changes. 
# Once you are satisfied that your username and email is correct, exit the command line by typing exit and hit enter.

# To link RStudio and Git:
# go to Tools, then Global Options, then Git/SVN. Click Ok or apply. Rstudio and Git are now linked. 

# Now, to link RStudio to GitHub in that same RStudio option window, click "Create RSA Key" and when there is complete, 
# click "Close". Following this, in that same window again, click "View public key" and copy the string of numbers and letters. 
# Close this window. Go to your account setting in Github

# Projects Under Version Control
# After creating a new repository in Github, click the Clone or download button and copy the repository link (for SSH). 
# In R Studio, go to file --> new project --> version control --> Git --> paste the URL and the select the location where you like the project
# to be stored --> create project (this project is now linked with Github repository) 
# Open an R script --> under Git Tab --> check the box under staged to stage your file --> commit --> a new window will open that lists
# all of the changed files from earlier and below that shows the differences in the staged files from previous versions 
# in the commit box write a commit msg --> Push 

# To link an existing R project with Git and Github 
# To open a new project (that is not linked with Github from the begining) --> file --> new project --> New Directory --> New project 
# Open terminal --> change directory (cd) to the project folder file --> git init --> git add .
# This initializes this directory as a Git repository and adds all of the files in the directory to your local repository.
# Commit these changes to the Git repository using --> git commit -m "initial commit"

# Now the project is linked with Git version control and to link this with Github:
# Go to Github.com create a new repository with the exact same name as the project and do not initialize a README file 
# Now you should see there is an option: push an existing repository from the command line, paste the lines of codes in terminal to link
# the repository to Github

# If there is an existing project that others are working on, you can link the existing project with R studio
# File --> new project --> version control with the URL of the project 

# R Markdown: creating fully reproducible documents in which both text and code can be combined 
# Since you can combine text and code in one document, you can integrate introductions, hypotheses, code, results and conclusions. 
# To create an R markdown: File -> new file -> R markdown , fill in the title, author and output format, when you click ok
# a new window open with explanation on R markdown 
# Text section, for example ## R Markdown will render as text when you produce the file and all of the formattng 
# generally applies to this section.
# The code chuncks by triple back ticks are ran from within the document and the output of the code will be included in output.
# When you are done with a document in R Markdown --> Knit and save the document at prompt as an RMD file 
# One of the huge benefits of R Markdowns is rendering the results to conde in line 

# To bold text,  surround it by two asterisks on either side. 

# To italicize text, surround the word with a single asterisk on either side. 

# To make section headers, put a series of hash marks. The number of hash marks determines what level of heading it is. 
# One hash is the highest level and will make the largest text. Two hashes is the next highest level and so on. 

# To make an R code chunk, you can type the three back ticks, followed by the curly brackets surrounding a lowercase r. 
# Put your code on a new line and end the chunk with three back ticks. 
# You can also use the shortcut Option + Command + I to generate the R code chunk in R Markdown. Also, along the top of the source
# quadrant, there is an insert button

# To make bulleted lists, proceed each perspective bullet point by a single dash, followed by a space. 
# Importantly, at the end of each bullets line, end with two spaces. 
# This is a quirk of R Markdown that will cause spacing problems if not included. 

# R Markdown cheat sheet
# The Leek group guide to data sharing: https://github.com/jtleek/datasharing

# P value tells you the probablity that the results of your experiment were observed by chance 
# p-value < 0.05 means that there is a five percent chance that the differences you saw were observed by chance. 
# (But if you do 20 tests by chance, you would expect one of the 20 that is five percent to be significant. 
# In the age of big data, testing 20 hypotheses is a very easy proposition, and this is where the term p-hacking comes from. 
# This is when you exhaustively search a dataset to find patterns and correlations that appear statistically significant 
# by virtue of the sheer number of tests you have performed. These spurious correlations can be reported as significant and 
# if you perform enough tests, you can find a dataset and analysis that will show you what you wanted to see.)

# Course 2 : R Programming 
# To get the working directory 
getwd()
# If you want to read a file, the file should be in your working directory, otherwie you will get an error

# R --> Misc --> change working directory

# To list all the files in the working directory 
dir()

# R comes with text editor (the empty page on the R top pane). You need to copy the code from here into R?

# You can create an empty vector with the vector function, which has two basic arguments: class of the object, lenght of the vector
x <- vector("numeric", length = 10)
x
# Inf stands for infinity, if you take one, divide it by zero, you'll get infinity and if you take 1 and divide it by infinity you'll get zero. 
# Another special value is called NaN, which represents an undefined value. For example, if you take zero over zero 
# that's not a number It's not defined so you'll get a NaN back 

# Matrix is a vector that has a dimension attribute to it...To create a matrix 
# Matrices are constructed column-wise. You can think of the matrix taking a vector and all the numbers are inserted into the 
# matrix by column
m <- matrix(1:6, nrow = 2, ncol = 3)
m
attributes(m)

# You can also create a matrix by creating the dimension attribute on a vector: 
m <- 1:10
dim(m) <- c(2, 5)
m

# Another way to create a matrix, is by binding columns or rows:
x <- 1:3
y <- 10:12
cbind(x, y)# column bind
rbind(x, y)# row bind 

# factor is a special type of vector, which is used to represent categorical data. 
# Types of factor: unordered (male and female) or ordered (assistant, associate and full professors)

# factors are treated specially by modeling functions like lm and glm, which are functions for fitting linear models
# factors can be created with the factor function and the input to the factor function is a character vector 

x <- factor(c("yes", "yes", "no", "yes", "no"))
x
table(x) # to see the frequency count of how many of each level there are
unclass(x) # strips out the class and bring it down to an integer vector
# and you can see that yes is coded as two and no is coded as one --> so factor it is really an integer vector with the 
# level attribute here no and yes 
# The order of the levels in the factor, can be set using the levels argument in factors.
# The baseline level is just the first level in the factor and it's determined using alphabetical order, hence no is the 
# baseline level here because n comes before y in the alphabet. If you like to have yes as the baseline level 
x <- factor(c("yes", "yes", "no", "yes", "no"), 
            levels = c("yes", "no"))
x
unclass(x)

# Missing values:
# Missing values in R are denoted by either NA or NAN 
# NaN is used for undefined mathematical operations and NA is pretty much used for everything else
is.na() # is used to test for NA
is.nan() # is used to test for NaN
# a NaN value is also NA but the converse is not true 

# Data frames are usually created by
x <- data.frame(foo = 1:4, bar = c("T", "T", "F", "F"))
x
read.table()
read.csv()

# matrices can have names using dimnames
m <- matrix(1:4, nrow = 2)
dimnames(m) <- list(c("a", "b"), c("c", "d"))
m
# To create a matrix from a data frame 
data.matrix()

# Reading Tabular Data
read.csv()
read.table()

readLines()# for reading lines of text file

source() # for reading R code files 
dget()# for reading R code files but it's for reading R objects that have been dparsed into text files.

load()# for reading binary objects into R 
unserialize()# for reading binary objects into R 

# The analogous functions for writing data to files
write.table()
write.csv()

writeLines()

dump()
dput()

save()
serialize()

# read.table is the mostly commonly used function for reading data into R (and into a data frame)
# read.table important arguments:
?read.table
# file, the name of a file or a connection. Usually you're going to give this a file name, it's going to be a string and it's
# going to be a path to a certain file in your computer 
# header, logical indiating if the file has a header
# sep, a string indicating how the columns are separated 
# colclasses, a character vector indicating the class of each column in the dataset
# If there are no commented lines in your file set comment.char = " "
# nrows, the number of rows in the dataset 
# comment.char, a character string indicating the comment character
# skip, the number of lines to skip from the begining 
# stringAsFactors, should character variables be coded as factors? The default is TRUE so if you don't want to read in factor 
# variable you should set this equal to FALSE 
read.csv("cards.csv")# read.csv specifies header to be equal to TRUE

# For reading larger datasets.
read.table() #This would read the entire data set in your computer's RAM, so you need to make 
# a rough calculation on how much memory you need to store the dataset you are about to read. 
# Specifying the colClasses argument (instead of using the default) can make read.table run MUCH faster. 
# In order to use this option, you have to know the class of each column in your data frame. 
# If all of the columns are "numeric", then you can just set colClasses = "numeric"

# A quick way to figure out the classes of each column 
initial <- read.table("datatable.txt", nrows = 100)# if it is a huge dataset, read in the first 100 rows
classes <- sapply(initial, class)# going through each of the columns using sapply and calling the class function to tell 
# you what class of data is in each column 
tabALL <- read.table("datatable.txt", 
                     colClasses = classes)# and then read the entire data set after by specifying colClasses argument 

# Set nrows. This doesn't make R run faster but it helps with memory usage. 

# Textual Data formats, which are different from tabular data and contain more meta-data
# dump() and dput() (source() and dget()) functions preserve metadata
# Textual formats can work much better with version control programs, which can only track changes meaningfully in text files
# Textual formats can be longer-lived; if there is corruption somewhere in the file, it can be easier to fix the problem
# Textual formats are not very space efficient 

y <- data.frame(a = 1, b = "a"#, stringsAsFactors = FALSE
)
y
dput(y)# to print into the console
dput(y, file = "y.R")# to save it to a file, the dput function essentially writes R code which can be used to 
# to reconstruct an R object
new <- dget("y.R")# to read it into R
new
# a b
# 1 1 a

# The dump function is like dput, however the difference is that dump can be used on multiple R objects
x <- "foo"
y <- data.frame(a = 1, b = "a")
dump(c("x", "y"), file = "data.R")# to pass dump a character object which contains the names of the objects
rm(x, y)
x
y
source("data.R")# to read those objects back into R 
x
y

# Connections: Interfaces to Outside World 
# Data are read in using connection interfaces. Connections can be made to files (most common) or to other more exotic things
# file, opens a connection to a file
# gzfile, opens a connection to a file compressed with gzip
# bzfile, opens a connection to a file compressed with bzip2
# url, opens a connection to a webpage 

str(file)# has a few arguments
# function (description = "", open = "", blocking = TRUE, encoding = getOption("encoding"), raw = FALSE, method = getOption("url.method", 
# "default")) 
# description is the name of the file
# open is code indicating: "r" read only, "w" writing and initializing a new file, "a" appending, 
# "rb", "wb", "ab" reading, writing, or appending in binary mode (Windows)

# In practice, we often don't need to deal with the connection interface directly
con <- file("foo.txt", "r")
data <- read.csv(con)
close(con)
# the code above is the same as below
data <- read.csv("foo.txt")

# However, using the connection is useful if you want to use parts of the file
con <- gzfile("words.gz")
x <- readLines(con, 10)# writeLines takes a character vector and writes each element one line at a time to a text file
x

con <- url("http://www.jhsph.edu", "r")
con
x <- readLines(con)# to read lines from this connection 
head(x)

# Subsetting-Basics
# [ always returns an object of the same class as the original; can be used to select more than one element

# [[ is used to extract elements of a list or a data frame; it can only be used to extract a single element and the class 
# of the returned object will not necessarily be a list or data frame

# $ is used to extract elements of a list or data frame by name; semantics are similar to that of [[

# Subsetting-Lists
x <- list(foo = 1:4, bar = 0.6)
x[1]
x[[1]]
x$foo
x$bar
x["bar"]# gives a list
x[["bar"]]

x[c(1, 2)]# to extract multiple elements of the list, then you need to use the single bracket operator 
# so the double bracket or dollar sign cannot be used to extract multiple elements

x[[c(1, 3)]]

# The [[ operator can be used with computed indices; $ can only be used wtih literal names
x <- list(foo = 1:4, bar = 0.6, baz = "hello")
name <- "foo"
x[[name]]# computed indices
# [1] 1 2 3 4
x$name
# NULL
x$foo
# [1] 1 2 3 4

# The [[ can take an integer sequence
x <- list(a = list(10, 12, 14), b = c(3.14, 2.81))
x[[c(1, 3)]]# 14
x[[1]][[3]]# 14
x[[c(2, 1)]]# 3.14

# Subsetting a Matrix
x <- matrix(1:6, 2, 3)
x  
x[1, 2] # subsetting a single element a matrix, we dont get a matrix but a vector 
# [1] 3
x[1, 2, drop = FALSE] # to get matrix (drop is defaulted to TRUE and drops the dimension so rather than getting a two 
# dimensional object back, you typically get a one-dimensional object back)
#      [,1]
# [1,]    3
x[2, 1]  
x[1, ] # also when you subset a single row or column, you do not get a matrix back and you get a vector back
# if you like to have a matrix back, then you have to use drop = FALSE argument 
x[ ,2]  

# Partial Matching, is allowed with [[ and $ 
x <- list(aardvark = 1:5)
x$a# here it looks for a name in this list that matches the letter a so aardvark here 
# [1] 1 2 3 4 5
x[["a"]]# the double bracket operator is looking for a name that is an exact match for one of the names in the list
# NULL
# so the double bracker operator does not do partial matching like the dollar sign does, unless you pass the double bracket
# a second argument
x[["a", exact = FALSE]]
# [1] 1 2 3 4 5

# Removing NA values 
x <- c(1, 2, NA, 4, NA, 5)
bad <- is.na(x)
bad
x[!bad]# bang operator 

#If there are multiple objects to subset with no missing values 
x <- c(1, 2, NA, 4, NA, 5)
y <- c("a", "b", NA, "d", NA, "f")
good <- complete.cases(x, y)
good
x[good]
y[good]

airquality[1:6, ]
good <- complete.cases(airquality) # tells which rows are complete 
# I am not sure here why complete.cases here tells us which rows are complete so in a way it is subsetting 
# the rows but not the columns 
airquality[good, ][1:6, ]

# Vectorized operations
x <- 1:4; y <- 6:9
x + y
x > 2
x >= 2
y == 8
x * y
x / y


x <- matrix(1:4, 2, 2); y <- matrix(rep(10, 4), 2, 2)
x
y
x * y # element-wise multiplication 
x / y

x %*% y # true matrix multiplication 

# Determine which directory your R session is using as its current working directory using getwd().
# List all the objects in your local workspace using ls().
# Some R commands are the same as their equivalents commands on Linux or on a Mac. 
# Both Linux and Mac operating systems are based on an operating system called Unix. 
# To list all the files in your working directory using list.files() or dir().
# Using the args() function on a function name is also a handy way to see what arguments a function can take.
old.dir <- getwd() # We will use old.dir at the end to move back to the place that we started. 

dir.create("testdir")# To create a directory in the current working directory called "testdir".
setwd("testdir")
file.create("mytest.R")# Create "mytest.R" file in your working directory
file.exists("mytest.R")# Checks if "mytest.R" exists in the working directory 
file.info("mytest.R")# Access information about the file "mytest.R" by using file.info().
# You can use the $ operator --- e.g., file.info("mytest.R")$mode --- to grab specific items.
file.rename("mytest.R", "mytest2.R")# Change the name of the file "mytest.R" to "mytest2.R" by using file.rename().
# The 'recursive' argument. In order to create nested directories, 'recursive' must be set to TRUE.
file.remove()# to delete files
file.copy("mytest2.R", "mytest3.R")# Make a copy of "mytest2.R" called "mytest3.R" using file.copy().
file.path()# provide the relative path. 
file.path("folder1", "folder2")# You can use file.path to costruct file and directory paths 

# Create a directory in the current working directory called "testdir2" and a subdirectory for it called "testdir3"
# In order to create nested directories, 'recursive' must be set to TRUE.
dir.create(file.path('testdir2', 'testdir3'), recursive = TRUE)

# if you have questions on an operator like the colon, you must enclose the symbol in backticks like this: ?`:`
# (if you don't have a backtick key, you can use regular quotes.)

# The most basic use of seq() does exactly the same thing as the `:` operator. Try seq(1, 20) to see this.
seq(1, 20)
seq(0, 10, by = 0.5)
my_seq <- seq(5, 10, length=30)
length(my_seq)

# To generate a sequence of integers from 1 to N, where N represents the length of the my_seq vector
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)

# One more function related to creating sequences of numbers is rep(), which stands for 'replicate'
rep(0, times = 40)
rep(c(0, 1, 2), times = 10)# repeating the vector (0, 1, 2)
rep(c(0, 1, 2), each = 10)# rather than repeating the vector (0, 1, 2), we want our vector to contain 10 zeros, then 10 ones, then 10 twos.


my_char <- c("My", "name", "is")
# my_char is a character vector of length 3. To join the elements of my_char together into one 
#  character string (i.e. a character vector of length 1), use paste() 
paste(my_char, collapse = " ")# Make sure there's a space between the double quotes in the `collapse` argument.
# "My name is"

paste("Hello", "world!", sep = " ")# the `sep` argument tells R that we want to separate the joined elements with a single space.

# One common scenario when working with real-world data is that we want to extract 
# all elements of a vector that are not NA (i.e. missing data).
# `!` gives us the negation of a logical expression, so !is.na(x) can be read as 'is not NA'.

# Control structures in R 
# Control structures allow you to control the flow of an R program:
# if, else: testing logical conditions
# for: executing a loop a fixed number of times
# While :executes a loop while a condition is true 
# Repeat: to execute an infinite loop
# Break: braek the execution of a loop 
# next: 
# return: exit a function 

# # Control structures: if else 
# if else allows you to test logical conditions, and to let the R program do something, depending on whether that conditions is true or false. 
# So if the condition is true then you do something, else you do something else. 
# The else part is optional, so you could just have an if statement to do something if a condition is true.
# But you can have the else part if you wanted to do something alternatively. 
# If there's more than one possible type of condition you can say if, and then, else if and then, else. 
# There can be any number of else if conditions, and the else one has to be at the end.

# Control structures: for loops
# you have a loop index which is typically called i
for (i in 1:10) {
        print(i)
}

x <- c("a", "b", "c", "d")
for (i in 1:4) {
        print(x[i])
}

for (i in seq_along(x)) {# seq_along takes a vector as an input and it creates an integer sequence that is equal to the lenght of 
        # of the vector 
        print(x[i])
}

# the  index variable does not have to be an integer. It can take elements from any arbitrary vector. 
for (letter in x) {
        print(letter)
}

for (i in 1:4) print(x[i])# if the for loop only have a single expression in its body the curly braces can be omitted 

# for loops can be nested so you can have a for loop inside of a for loop
# Nesting beyond 2-3 levels is very difficult to read/understand 
x <- matrix(1:6, 2, 3)
for(i in seq_len(nrow(x))) {
        for(j in seq_len(ncol(x))){# seq_len takes an integer and creates an integer sequence out of that
                print(x[i, j])
        }
}

# Control structures: while loops
# while takes a logical expression and will execute the loop based on the value of that logical expression. 
# The below code initializes a count variable equal to 0. And then while that count is less than 10 it prints out the count and 
# then increments the count by 1. So, as soon as the, the value of count gets to 10 the loop stops. 

count <- 0
while(count < 10) {
        print(count)
        count <- count + 1
}

# While loops can potentially result in infinite loops if not written properly. 
# For example, it is a bit hard to tell when the below while loop will finish. Becuase the body of the loop involves 
# random number generation 
z <- 5
while (z >= 3 && z <= 10) {
        print(z)
        coin <- rbinom(1, 1, 0.5)#flip a fair coin 
        
        if(coin == 1) { ## random walk, if 1 I'll add 1 to the value of z and if not subtract 1 from z
                z <- z + 1
        } else {
                z <- z - 1
        }
}

# Control structures: repeat
# repeat initiates an infinite loop. The only way to exit a repeat loop is to call a break. 
x0 <- 1
tol <- 1e-8
repeat {
        x1 <- computeEstimate()#this function is an imaiginary function
        if(abs(x1 - x0) < tol) {
                break
        } else {
                x0 <- x1
        }
}
# With this loop, there is no guarantee that it will stop. It is better to set a hard limit on the number of iterations 
# (using a for loop)

# Control structure: next, return
# next is used to skip an iteration of a loop (break is a way to exit the loop entirely)
# (return is used to exit a loop/function, it is primarily used to exit a function, it will exit the entire function 
# and return a value that you pass it)
for (i in 1:100) {
        if (i <= 20) {
                # skip the first 20 iterations
                next
        }
        # Do sth here 
}
# Control structures mentioned above are primarily useful for writing progrms; for command-line interactive work, there are other 
# looping type functions that can be used and they generally have the word *apply in them 

# Your first R function 
# the function returns whatever the last expression was 
add2 <- function(x, y){
        x + y
}
add2(3, 5)#8

above10 <- function(x) {
        use <- x > 10# constructs a logical statements that figures out which elements of the vector x are greater than 10
        # so this will return a logical vector of TRUEs and FALSEs
        x[use]
}

above <- function(x, n = 10) { # you would construct a function that would allow to extract the elements of a vector that are 
        # above an arbitrary number but you are defaulting to 10 here 
        use <- x > n
        x[use]
}

x <- 1:20
above(x, 12)

x <- 1:6
x <- matrix(x, nrow = 2)

columnmean <- function(x, removeNA = TRUE) {# we add an argument here removeNA to default to TRUE and we pass this argument to 
        # the mean function and this would default to remove NAs when calculating the column means 
        nc <- ncol(x)
        means <- numeric(nc)# initialize a vector that will store the means for each column, the lenght of the vector has to 
        # equal the number of columns 
        
        for(i in 1:nc) {
                means[i] <- mean(x[,i], na.rm = removeNA)
        }
        means
}

columnmean(x)
columnmean(airquality)# first two columns are NAs
columnmean(airquality, removeNA = FALSE)

# functions are R objects that are of class function 
f <- function(<arguments>){
    ## Do sth interesting     
}
# functions are first class R objects: 
# functions can be passed as arguments to other functions 
# functions can be nested, so that you can define a function inside another function
# The return value of a function is the last expression in the function body to be evaluated 
# functions have named arguments which potentially have default values 
# the formal argument are the arguments included in the function definition 
# the formals function returns a list of all the foraml arguments of a function 

# Argument matching:
# 1. check for an exact for a named argument
# 2. check for a partial match, as long as there is unique match
# 3. check for a positional match

my_data <- rnorm(100)#simulating a 100 normal random variables 
sd(my_data)#calculating standard deviation 

# Defining a function 
f <- function(a, b = 1, c = 2, d = NULL) {
        
}# when defining a function, you should specify the name of the 
# argument and whether or not it has a default value
# NULL is a common value to assign to an argument, which usually means that there is nothing in there

# One of the key features of R is lazy evaluation: the arguments are only evaluated if needed
f <- function(a, b) {
        a^2
}
f(2)
# This function never uses the argument b, the argument is never evaluated. So calling f(2) will not produce an error
# becuase 2 gets positionally matched to a

f <- function(a, b) {
        print(a)
        print(b)
}
f(45)
# [1] 45
# Error in print(b) : argument "b" is missing, with no default
# 45 got printed first before the error was triggered -- > lazy evaluation 

# "..." argument
# indicate a variable number of arguments that can sometimes be passed on to other functions 
# it is often used when extending another function and you don't want to copy the entire argument list of the original function 
# to extend the plot function, and to change some of the defaults, you create a function myplot
# myplot replicate some of the arguments of the original plot function like x and y
# but it is going to change the default type argument so that instead of creating circles for point, create lines 
# ... is passed down to the original plot function, and so all of the original arguments are preserved 
myplot <- function(x, y, type = "l", ...) {
        plot(x, y, type = type, ...)
}

# There is another use of ... and it is used for generic functions so that extra arguments can be passed to the methods 
mean
# function (x, ...) 
# UseMethod("mean")
# generic functions don't do anything, but they dispatch methods for different types of data. And so the dot dot dot is used very heavily in this type of setup.

# "..." argument is necessary when the number of arguments that are passed to a function cannot be known in advance. 
# One good example of this usage is in the paste function. paste, concatenates a set of strings together to create one string or a vector of strings 
# and it can take a variable number of arguments. So, there is no way for the function to note in advance how many arguments it's going to have to paste together
# and so the first argument for paste is actually dot dot dot. And therefore you can take a number of different R objects that are character vectors and then, 
# paste together using a separator 
# Another function that has the dot dot dot as the first argument is cat. And what cat does similar to paste, it puts together a number of strings 
# then it prints out the concatenated string either to a file or to a console. 
# So there are other arguments to paste and cat but the first argument is going to be the set of our objects that are going to be concatenated.
paste()
cat()
# Any arguments that appear after ... on the argument list must be named explicitly and in full and cannot be partially matched. So you can not use
# positional or partial matching for arguments that come after ... 
paste("a", "b", sep = ":")# "a:b"
paste("a", "b", se = ":")# "a b :"

# Another strict rule: all arguments after an ellipses must have default values.

# How to "unpack" arguments from an ellipses when you use the
# ellipses as an argument in a function. Below I have an example function that
# is supposed to add two explicitly named arguments called alpha and beta.
# 
# add_alpha_and_beta <- function(...){
#   # First we must capture the ellipsis inside of a list
#   # and then assign the list to a variable. Let's name this
#   # variable `args`.
#
#   args <- list(...)
#
#   # We're now going to assume that there are two named arguments within args
#   # with the names `alpha` and `beta.` We can extract named arguments from
#   # the args list by using the name of the argument and double brackets. The
#   # `args` variable is just a regular list after all!
#   
#   alpha <- args[["alpha"]]
#   beta  <- args[["beta"]]
#
#   # Then we return the sum of alpha and beta.
#
#   alpha + beta 
# }

# Scoping Rules - Symbol Binding 
search()
# When r tries to bind a value to a symbol, it searches through a series of environments to find the appropriate value. 
# 1. Search the global environment for a symbol name matching the one requested 
# 2. Search the namespaces of each of the packages on the search list. So the search list consists of all the R packages that are currently loaded into R. 
# And so there's an order to the search list. It starts at the global environment. The second on the search list is the stats package, the graphics package, the GR devices package. 
# the base package is always the last element on the search list 
# The order of the packages on the search list matters
# When a user loads a package with a library function, the namespace of that package which is the environment that has all the symbols and 
# all the values for the symbols, gets put in the second position of the search list. So right behind the global environment. 
# And then everything else just kind of get pushed down one level.

# R has separate namespaces for functions and non-functions, so it is possible to have an object named c somewhere and the function name c. 
# Of course, in your global environment, there can only be one symbol named c. 

# The scoping rules for R are the main feature that make R different from the original S language 
# The scoping rules determine how a value is associated with a free variable in a function. In a function there are two types of variables
# there is the function arguments that are passed through the definition of the function and there maybe other variables that are 
# not functions arguments and the question is how do you assign a value to these variables?
# R uses lexical scoping or static scoping. A common alternative is dynamic scoping
# Related to the scoping rules is how R uses the search list to bind a value to a symbol
f <- function(x, y) {
        s^2 + y / z# here z is the free variable, and the question is what value do we assign to z and this is determined by the scoping rules
}
# Lexical scoping: the values of free variables are searched for in the environment in which the function was defined 
# An environment is a collection of symbol and value pairs 
# Every environment has a parent environment; it is possible for an environment to have multiple children 
# Each package has a namespace, and that's like an environment. It has a bunch of symbols and values associated with it.
# The only environment without a parent is the empty environment 
# Typically the function is defined the global environment so that values of the free variables are just found in the user's workspace. 
# So the idea here is that you can define things like global variables, that will be common to a lot of different functions. That you might be defining in your workspace.

make.power <- function(n) {
        pow <- function(x) {
                x^n
        }
        pow
}
# this is a constructer function so the function is constructing another function 

cube <- make.power(3)
square <- make.power(2)
cube(3)#27
square(3)#9

# Exploring a function closure (how do you know what is in a function's environment)
ls(environment(cube))# "n"   "pow"
get("n", environment(cube))# 3

y <- 10
f <- function(x) {
        y <- 2
        y^2 + g(x)# y and g here are free variables 
}

g <- function(x) {
        x^y# y is a free variable, with lexical scoping the value of y in the function g is looked up in the gloval environment so it is 10
}# However with dynamic scoping, the value of y is looked up in the environment from which the function was called (referred to as
# calling environment. In R the calling environment is known as the parent frame.)

f(3)

# Consequences of Lexical Scoping
# In R, all objects muct be stored in memory.  


# Optimization routines in R: optim, nlm and optimize. 
# Command I for indenting 

traceback()
debugger()

# Dates and Times in R 
# Dates are represented by the Date class
# Dates are stored internally as the number of days since 1970-01-01 and do not have time attached to them
# you can take a character screen and convert it into a date:
x <- as.Date("1970-01-01")# if you print it, it looks like a character string but it is not
x
unclass(x)# 0
x <- as.Date("1970-01-02")
x
unclass(x)# 1

# Times are represented by the POSIXct or the POSIXlt class
# Times are stored internally as the number of seconds since 1970-01-01
# In the POSIXct class, time is represented as a large integer 
# POSIXlt stores time as a list, and it stores other useful information like what is the day of the week, year, month or the month itself

# There are a number of generic functions that work on dates and times 
# So these generic functions work on the class POSIXct, POSIXlt, or date
weekdays()# give the day of the week
months()# give the month name
quarters()#give the quarter number

# You can coerce back and forth between POSXIct and lt using as.POSIXct or as.POSIXlt
x <- Sys.time()# this function just gives you the current time
x
class(x)# "POSIXct" "POSIXt" 
unclass(x)# gives me the number of seconds since January 1st 1970
names(unclass(x))
p <- as.POSIXlt(x) 
p
unclass(p)
names(unclass(p))
p$sec

# strptime function converts dates which are written in character string format into date or time objects. 
datestring <- "December 9, 2011 9:10"
x <- strptime(datestring, "%B %d, %Y %H:%M")# this is a format string
x
?strptime
class(x)# "POSIXlt" "POSIXt"
x <- as.Date("2012-01-01")
y <- strptime("9 Jan 2011 11:34:21", "%d %b %Y %H:%M:%S")
x-y# Incompatible methods ("-.Date", "-.POSIXt") for "-" 
# Error in x - y : non-numeric argument to binary operator
x <- as.POSIXlt(x)
x-y# Time difference of 356.3095 days

# date and time operators keep track of tricky things like leap years, leap seconds, daylight savings and time zones
x <- as.POSIXct("2012-10-25 01:00:00")
y <- as.POSIXct("2012-10-25 06:00:00", tz = "GMT")
y-x
# Character strings can be coerced to Date/Time classes using strptime() function or the as.Date, as.POSIXct or as.POSIXlt
# A lot of plotting options recognize date and time class

#  In order to negate boolean expressions you can use the NOT operator. 
# An exclamation point `!` will cause !TRUE (say: not true) to evaluate to FALSE 
# and !FALSE (say: not false) to evaluate to TRUE.
!(5 == 7)# TRUE

# You can use the `&` operator to evaluate AND across a vector. 
# The `&&` version of AND only evaluates the first member of a vector. non-vectorized
# The `|` version of OR evaluates OR across an entire vector.
# The `||` version of OR only evaluates the first member of a vector. non-vectorized 
# All AND operators are evaluated before OR operators:
5 > 8 || 6 != 8 && 4 > 3.9
# First the left and right operands of the AND operator are evaluated.
# 6 is not equal 8, 4 is greater than 3.9, therefore both operands are TRUE 
# so the resulting expression `TRUE && TRUE` evaluates to TRUE. 
# Then the left operand of the OR operator is evaluated: 5 is not greater than 8 
# so the entire expression is reduced to FALSE || TRUE. 
# Since the right operand of this expression is TRUE the entire expression evaluates to TRUE.

# The function isTRUE() takes one argument. If that argument evaluates to TRUE, the function will return TRUE.
# Otherwise, the function will return FALSE. 

# The function identical() will return TRUE if the two R objects passed to it as arguments are identical. 

# You should also be aware of the xor() function, which takes two arguments. 
# The xor() function stands for exclusive OR. 
# If one argument evaluates to TRUE and one argument evaluates to FALSE, then this function will return TRUE, 
# otherwise it will return FALSE. 
xor(5 == 6, !FALSE)
# if the firs argument was changed to 5 == 5 and the second argument was unchanged, would have evaluated to FALSE.

# The which() function takes a logical vector as an argument and returns the indices of the vector that are TRUE. 
# For example which(c(TRUE, FALSE, TRUE)) would return the vector c(1, 3).
ints <- sample(10)
which(ints > 7)# 4 6 9

# The any() function will return TRUE if one or more of the elements in the logical vector is TRUE. 
# The all() function will return TRUE if every element in the logical vector is TRUE.

Sys.Date()#returns a string representing today's date
# Inputs to functions are often called arguments.

# The syntax for creating new binary operators in R is unlike anything else in
# R, but it allows you to define a new syntax for your function. I would only
# recommend making your own binary operator if you plan on using it often!
#
# User-defined binary operators have the following syntax:
#      %[whatever]% 
# where [whatever] represents any valid variable name.
# 
# Let's say I wanted to define a binary operator that multiplied two numbers and
# then added one to the product. An implementation of that operator is below:
#
# "%mult_add_one%" <- function(left, right){ # Notice the quotation marks!
#   left * right + 1
# }
#
# I could then use this binary operator like `4 %mult_add_one% 5` which would
# evaluate to 21.

# Loop functions 
# Writing for, while loops is useful when programming but not particularly easy
# when working interactively on the command line. There are some functions which 
# implement looping to make life easier:
# lapply: loop over a list and evaluate a function on each element 
# sapply: Same as lapply but try to simply the result
# apply: Apply a function over the margins of an array (useful when taking summaries of matrices or higher dimensional arrays)
# tapply: Apply a function over subsets of a vector (short for table apply)
# mapply: Multivariate version of lapply 
# An auxillary function split is also useful, particularly in conjugation with lapply or sapply, it splits objects into sub-pieces 

# lapply
# takes three arguments: (1) a list X; (2) a function (or the name of a function) FUN;
# (3) other arguements that can be passed to the ... argument. 
# The ... is used to pass arguments that go with the function that being applied to each element of the list. 
# If x is not a list, it will be coerced to to a list using as.list. If it can not coerce to a list, gives an error.
# lapply always returns a list, regardless of the class of the input. 
X <- list(a = 1:5, b = rnorm(10))# rnorm creates random 10 normal variables
lapply(X, mean)

x <- list(a = 1:4, b = rnorm(10), c = rnorm(20, 1), d = rnorm(100, 5))
x
lapply(x, mean)

x <- 1:4
lapply(x, runif)# runif generates uniform random variables using random number generator
# runif(1) is going to generate 1 random variable 

lapply(x, runif, min = 0, max = 10)# this is to change the default values of the arguments of runif
# so we can pass those arguments to the ...
# lapply and friends make heavy use of anonymous functions 
# anonymous functions are functions that do not have names 
x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2))
x
# I want to extract the first column for each one these matrices
lapply(x, function(elt) elt[,1])# here elt is the argument of the function
# This function does not exist, except within the context of lapply and after the lapply is finished
# the function goes away, so that is an anonymous function 

# sapply is just a variant of lapply and tries to simplify the result of lapply if possible
# lapply always returns a list, if the result is a list where every element is length 1, then sapply will return a vector 
# if the results is a list where every element is a vector of the same length (>1), a matrix is returned
# if sapply can not figure out things, a list is returned

# apply
# used to evaluate a function (often an anonymous one) over the margins of an array
# it is most often used to apply a function to the rows or columns of a matrix 
# it can be used with general arrays, e.g. taking the average of an array of matrices 
# it is not really faster than writing a loop, but it works in one line 
str(apply)
# function (X, MARGIN, FUN, ...)  
# X is an array 
# MARGIN is an integer vector indicating which margins should be "retained"
# FUN is a function to be applied
# ... is for other arguments to be passed to FUN

x <- matrix(rnorm(200), 20, 10)#rnorm generates normal random variables
apply(x, 2, mean)# to calculate the mean of each column
# the MARGIN here is 2 which is column so I get back a vector of length 10 
# that has the mean of each column of the matrix

apply(x, 1, sum)# in a way here we preserve the rows and collapse the columns
# for each row we calculate the sum of that row

# for sums and means of matrix dimensions, we have some shortcuts
rowSums()# apply(x, 1, sum)
rowMeans()# apply(x, 1, mean)
colSums()# apply(x, 2, sum)
colMeans()# apply(x, 2, mean)
# These shortcut functions are equivalent to using apply function but they are much faster

x <- matrix(rnorm(200), 20, 10)
apply(x, 1, quantile, probes = c(0.25, 0.75)/100)# the output is a matrix 2 by 20
a
a <- array(rnorm(2 * 2 * 10), c(3, 2, 2))
apply(a, c(1, 2), mean)# here we are keeping the first and second dimension of the array
# it will collapse the third dimension or in a way average over the third dimension and give the mean matrix
# the other way to do this is with rowMeans which can be used with an array
rowMeans(a, dims = 2)

# mapply 
# is a multivariate version of lapply and sapply
# Attention: lapply, sapply and tapply only apply a function over the elements of a single object such as list
# so when you have more than one list to apply a function to then you can you use maaply 
# mapply can take multiple list arguments and then apply a function to the elements of those multiple lists in parallel
mapply# it applies a function in parallel over a set of different arguments 
# function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) 
# FUN is a function to apply
# ... contain arguments to apply over which are list 
# MoreArgs is a list of other arguments to FUN
# SIMPLIFY indicates whether the result should be simplified, this argument also exist in sapply and tapply
list(rep(1, 4), rep(2, 3), rep(3,2), rep(4,1))#rep is the repeat function
mapply(rep, 1:4, 4:1)# this will do the same as the above code

noise <- function(n, mean, sd) {
    rnorm(n, mean, sd)
}
noise(5, 1, 2)
mapply(noise, 1:5, 1:5, 2)# so mapply is allowing for vecror arguments 
list(noise(1, 1, 2), noise(2, 2, 2),
     noise(3, 3, 2), noise(4, 4, 2),
     noise(5, 5, 2))
 
# tapply
# is used to apply a function over subsets of a vector.
str(tapply)
# function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)  
# x is a vector
# INDEX is a factor or a list of factors (or else they are coerced to factors) 
# (another vector of the same lenght which identifies which group each element of the numeric vector is in)
# FUN is a function to be applied
# ... contains other arguments to be passed to FUN
# simplify, should we simplify the results? 

x <- c(rnorm(10), runif(10), rnorm(10, 1))#10 normals that have a mean of 1 
x
f <- gl(3, 10)# creating a factor variable using gl function. This factor variable 
# is going to have three levels and each level is going to be repeated 10 times 
# so the factor variable indicates which group the observation is in 
f
tapply(x, f, mean)# take the mean of each group of numbers in x

# if you don's simplify the results then what you get back is a list
tapply(x, f, mean, simplify = FALSE)

tapply(x, f, range)#this gives me the min and max
# so tapply is useful because it splits up a vector into little pieces and it applies a summary 
# statistic or function to those little pieces, and then after it applies a function it brings the 
# pieces back together again. 

# split takes a vector or other objects and splits into groups determined by a factor or list of factors
str(split)
# function (x, f, drop = FALSE, ...)  
# x is a vector (or list) or data frame 
# f is a factor (or coerced to one) or a list of factors
# drop indicates whether empty factor levels should be dropped 
# Once ou have x split apart, you can use lapply, or sapply to apply a function to those individual groups

x <- c(rnorm(10), runif(10), rnorm(10, 1))
f <- gl(3, 10)
split(x, f)# split always returns a list back 
lapply(split(x, f), mean)# it is common to use the lapply function with the split function 
# in the above expample we could use tapply

# With split, we can split much more complicated types of objects
library(datasets)
head(airquality)
# to calculate the mean of Ozone, Solar Radiaiton and Tempreture within each month
# so we have to split the dataframe into monthly pieces 
s <- split(airquality, airquality$Month)
s

lapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))# returns a list
sapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))# returns a matrix
sapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")], na.rm = TRUE))# to remove the NAs before
#calculating the mean 

# Splitting on more than one level
# For example, you might have a factor that is gender (male and female)
# and another that is race. And, you might want to look at the combination of the levels within those factors. 
s <- rnorm(10)
f1 <- gl(2, 5)# a factor of 2 levels each repeated 5 times
f2 <- gl(5, 2)# a factor of 5 levels each repeated 2 times
f1
f2
interaction(f1, f2)# to combine all the levels of the first one with all the levels of the second one 
# because there are two levels in the first factor and five in the second one
# there would be a total combinations of 10 levels 
# [1] 1.1 1.1 1.2 1.2 1.3 2.3 2.4 2.4 2.5 2.5
# Levels: 1.1 2.1 1.2 2.2 1.3 2.3 1.4 2.4 1.5 2.5

str(split(x, list(f1, f2)))# list of 10# so when using a split function we do not have to use the 
# interaction function, we can just pass it a list with the two factors
# and it will automatically call the interaction function and create the 10 level interaction factor
# it returns a list with the 10 different interaction factor levels and the elements 
# of the numeric factors that are within those 10 levels 
# there are empty levels here and here is how to drop the empty levels that are created by the splitting:
str(split(x, list(f1, f2), drop = TRUE))# list of 6

# Debugging Tools
# They are useful for figuring out what is wrong after you've discovered there is a problem
# Indications that something is not right:
# message: A generic notification/diagnostic message produced by the message function; 
# execution of the function continues 

# warning: An indication that something unexpected happened which might not necessarily be a problem

# error: An indication that a fatal problem has occurred; execution stops; produced by the stop function

# condition: A generic concept for indicating that something unexpected can occur;
# programmers can create their own conditions 

# Warning
log(-1)
# [1] NaN
# Warning message:
# In log(-1) : NaNs produced

# invisible() is a function that prevents auto printing. 
# load function loads objects from a saved workspace, so it's like the opposite of save
# when it loads, it actualy returns a character vector containing the names of 
# all the objects that it loads but that does not get printed becuase it's returned invisibly 

printmessage <- function(x) {
    if (x > 0)
        print("x is greater than zero")
    else
        print("x is less than or equal to zero")
    invisible(x)
}
printmessage(1)
printmessage(NA)

# Debugging Tools in R
# The primary tools for debugging sunctions in R are:
# traceback: prints out the function call stack after an error occurs; does nothing if there's no error
# so traceback tells you how many function calls you are in and where the error occurred 

# debug: you give it a function as an argument and it flags the function for "debug" mode
# which means that anytime the function is executed, it will halt the execution of the function 
# at the first line in a browser and then you can step through and execute the function line by line 
# to find out the specific line of code where the error occurs 

# browser: you can stick the browser function anywhere in your code and
# when that line of code gets hit,the browser call gets hit, the execution of the function will suspend
# and then you can go line by line from there.
# the debug function always start the debugging and the browser right at the top of the function. 
# But sometimes, you kind of want to run through the beginning and then stop it somewhere in the middle. 
# And the browser function allows you to stick the browser call anywhere in your code 
# and then it will run the function up until that point and then suspend it.

# trace: allows you to insert debugging code into a function without actually editing the function itself

# recover is an error handler function 
# normally when you get an error, you usually get a message saying what the error was 
# the execution of that function stops and you get the console back. 
# You can change that default behavior by setting an error handler. 
# And recover is an error handler function which means that any time you encounter an error, 
# anywhere in a function, the R interpreter will stop the execution of the function 
# right where the error occurred, and will kind of freeze it there. And then it will print out the function call stack, 
# Now you're in the browser, and you can look around in the different function calls 

traceback()#should be used right after the error
debug()#it will print out the entire code of the function 
# you get the browse prompt and now you are in a browser
# the environment of the browser is the environment of the function 
# so the things that are in the function are in the browser environment 
# pressing n for next and enter and it would run the first line and then the rest until the line that error occured 
# and that line will give us an error

options(error = recover)
# this is to set the recover to be the error handler 
# this will set a global option, as long as your R session is open and when you quit it will go away 
# you get a menu back, which is the function call stack (it is the same thing that you would get by traceback)

# Whereas sapply() tries to 'guess' the correct format of the result, vapply() allows you to specify it explicitly. 
# If the result doesn't match the format you specify, vapply() will throw an error, causing the operation to stop. 
# This can prevent significant problems in your code that might be caused by getting unexpected return values from sapply().
vapply(flags, unique, numeric(1))# which says that you expect each element of the result to be a numeric vector of length 1.
# You might think of vapply() as being 'safer' than sapply(), since it requires you to specify the format of the output in
# advance, instead of just allowing R to 'guess' what you wanted. In addition, vapply() may perform faster than sapply() for large
# datasets. However, when doing data analysis interactively (at the prompt), sapply() saves you some typing and will often be good
# enough.
tapply(flags$animate, flags$landmass, mean)# to apply the mean function to the 'animate' variable separately 
# for each of the six landmass groups

# These below three functions do the same thing
tapply(mtcars$mpg, mtcars$cyl, mean)
with(mtcars, tapply(mpg, cyl, mean))
sapply(split(mtcars$mpg, mtcars$cyl), mean)

# R Programming 
# the <<- operator can be used to assign a value to an object in an environment that is different from the current environment.
# Below are two functions that are used to create a special object that stores a numeric vector and cache's its mean.
# The first function, makeVector creates a special "vector", which is really a list containing a function to
# 1. set the value of the vector
# 2. get the value of the vector
# 3. set the value of the mean
# 4. get the value of the mean

makeVector <- function(x = numeric()) {
    m <- NULL
    set <- function(y) {
        x <<- y
        m <<- NULL
    }
    get <- function() x
    setmean <- function(mean) m <<- mean
    getmean <- function() m
    list(set = set, get = get,
         setmean = setmean,
         getmean = getmean)
}

# The following function calculates the mean of the special "vector" created with the above function. 
# However, it first checks to see if the mean has already been calculated. 
# If so, it gets the mean from the cache and skips the computation. 
# Otherwise, it calculates the mean of the data and sets the value of the mean in the cache via the setmean function.

cachemean <- function(x, ...) {
    m <- x$getmean()
    if(!is.null(m)) {
        message("getting cached data")
        return(m)
    }
    data <- x$get()
    m <- mean(data, ...)
    x$setmean(m)
    m
}

# Assignment: Caching the Inverse of a Matrix
# Matrix inversion is usually a costly computation and there may be some benefit to caching the inverse of a matrix rather 
# than compute it repeatedly (there are also alternatives to matrix inversion that we will not discuss here). 
# Your assignment is to write a pair of functions that cache the inverse of a matrix.
# Write the following functions:
# makeCacheMatrix: This function creates a special "matrix" object that can cache its inverse.
# cacheSolve: This function computes the inverse of the special "matrix" returned by makeCacheMatrix above.
# If the inverse has already been calculated (and the matrix has not changed), then the cachesolve should retrieve the inverse from the cache.

# Computing the inverse of a square matrix can be done with the solve function in R. 
# For example, if X is a square invertible matrix, then solve(X) returns its inverse.

# A cache is a way to store objects in memory to accelerate subsequent access to the same object.

# Simulation & Profiling
# profiler lets you collect detailed information on how your R functions are running
# and to identify bottlenecks that can be addressed. It is a key tool in helping you
# optimize your programs.

# the str function
# answers the question of what is in this R object 
# A dignostic function and an alternative to summary
# it is especialy well suited to compactly display the (abbreviated) contents of
# (possibly nested) lists.

str(str)# function (object, ...)  
# It is a function that takes an object
str(lm)# it gives you the function arguments for the lm function 
# function (formula, data, subset, weights, na.action, method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
# contrasts = NULL, offset, ...) 
str(ls)
x <- rnorm(100, 2, 4)

summary(x)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -6.883  -0.389   1.906   1.996   4.163  13.337 

str(x)# it will tell you that x is a numeric vector. There are 100 elements and gives you the first five elements
# num [1:100] 2.382 7.275 -0.229 -6.883 2.295 ...

f <- gl(40, 10)# to create a factor variable with 40 levels and each repeated 10 times 
str(f)# so it tells that it is a factor with 40 levels, the first 4 levels are named 1, 2, 3, 4
# and the first couple of elements in this factor all have the label one 
# Factor w/ 40 levels "1","2","3","4",..: 1 1 1 1 1 1 1 1 1 1 ...
summary(f)# it gives the number of elements at each of the 40 different levels 
# 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 
# 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 

library(datasets)
head(airquality)
str(airquality)

m <- matrix(rnorm(100), 10, 10)
str(m)# it tells that it is a matrix (a two dimensional array 10 rows and 10 columns)
# and will show the first column
# num [1:10, 1:10] -0.5058 0.5208 -2.3599 0.6561 0.0933 ...

s <- split(airquality, airquality$Month)
str(s)# this is a list that contains 5 different data frame each correponding to one mont

# Simulation - Generating Random Numbers 
# Generating Random Numbers
# Functions for probability distributions in R
# rnorm: generate random normal variates with a given mean and standard deviation 
# dnorm: evaluate the normal probability density (with a give mean/SD) at a point (or vector of points)
# pnorm: evaluate the cumulative distribution function for a normal distribution 
# rpois: generate random Poisson variates with a given rate
# qpois: inverse cumulative distribution function for the Poisson distribution

# Probability distribution functions usually have four functions associated with them:
# d for density 
# r for random number generation 
# p for cumulative distribution 
# q for quantile function 

# So every distribution has these four types of functions. For exmple for the gamma distribution, 
# there'll be a dgamma, rgamma, pgamma, and a qgamma function. 
# And for the Poisson distribution there's the rpoise dpoise ppoise, and qpoise functions.

# All the functions require that you specify the mean and standard deviation,
# because that's what specifies the actual probability distribution. 
# If you do not specify them, then the default values are a standard normal distribution, 
# which has a mean zero and standard deviation one. 

dnorm(x, mean = 0, sd = 1, log = FALSE)# to allow you to evaluate the log of the density 
# the default value is false 

pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
# whether or not you want to evaluate the lower.tail of the distribution, which is the default here  
# the lower tail is the part of the probability distribution that goes to to the left 
# if you want to evaluate the upper tail of distribution, then you want to equal the lower trail to false
# and that would evaluate the upper tail of the distribution 

rnorm(n, mean = 0, sd = 1)# n is the number of random variables that you want to generate 
x <- rnorm(10)
x
x <- rnorm(10, 20, 2)
x
summary(x)# the mean is roughly 20 

# Always set the random number seed when conducting a simulation 
# So anytime you simulate random numbers from any distribution, it is very important to 
# set the random number generator seed
# if you want to generate the same set of random numbers:
set.seed(1)# here we set the seed, it sets the sequence of random variables that is going to occur 
rnorm(5)

rnorm(5)# different than previous output

set.seed(1)# here we reset the seed to 1 and draw 5 again it will be the same as the first draw
# so by reseting we set the sequence to go back to where we started and allows for you 
# to reproduce random numbers that you generate 
rnorm(5)

# generate random variables from Poisson distribution
rpois(10, 1)# 10 Poisson random variables with a rate of 1
# for the Poisson distribution the mean is equal to rate 
ppois(2, 2)# evaluate the cumulative distribution for the Poisson distribution
# what is the probability that a Poisson random variable is less than or equal to two 
# if the rate is 2 

# Simulation - simulating a linear model 

# So I've got a fairly simple linear model. It has a single predictor, x and it's going to have random noise, what I call epsilon that, 
# has a normal distribution with standard deviation two.
set.seed(20)
x <- rnorm(100)
e <- rnorm(100, 0, 2)
y <- 0.5 + 2 * x + e# regression coefficients: intercept beta knot (0.5 here) and slope beta one (2 here)
summary(y)
plot(x, y)# x and y have a linear relationship that follows the model that we specified 

# what if x is a binary random variable maybe it represents gender or treatment versus control 
set.seed(10)
x <- rbinom(100, 1, 0.5)# to generate 100 binomial random variables with binomial distribution of 
# n equal to 1 and p equal to 0.5 
# so the probability of 1 is going to be equal to 0.5
e <- rnorm(100, 0, 2)# error
y <- 0.5 + 2 * x + e# so the x variable is binary and the y variable is continuous and normal 
summary(y)
plot(x, y)

# Generalized linear model such as Poisson distribution 
# for example perhaps you want to simulate outcome data are count variables instead of continuous variables  
# here the error distribution is not going to be normal, it's going to be a Poisson distribution 
# so let's assume that the outcome y has a Poisson distribution with mean mu
# and that the log of mu follows a linear model with a intercept beta knot (0.5 here) and 
# a slope beta one (0.3 here) so x is going to be one of our predictors
set.seed(1)
x <- rnorm(100)
log.mu <- 0.5 + 0.3 * x# to generate the linear predictor log of mu 
y <- rpois(100, exp(log.mu))# in order to the get the mean for the Poisson random variable, we need to exponentiate that 
summary(y)
plot(x, y)# there is a linear relationship between x and y, as x increases the count for y generally gets larger

# Simulation - random sampling 
sample()

# Simulation 
# Drawing samples from specific probability distributions can be done with r* functions
# Standard distributions are built in: Normal, Poisson, Binomial, Exponential, Gamma, etc
# The sample function can be used to draw random samples from arbitrary vectors
# Setting the random number generator seed via set.seed is critical for reproducibility 

# R profiler 
# The profiler in R is a handy tool for when you are developing larger programs or doing big data analyses
# the profiler is a really handy tool to figure out exactly why it's taking so much time and suggest strategies for fixing your problem

# Why is my code so slow?
# Profiling is a systematic way to examine how much time is spent in different parts of a program
# Useful when trying to optimize your code
# Often code runs fine once, but what if you have to put it in a loop for 1,000 iterations? Is it still fast enough?
# Profiling is better than guessing 
# Getting biggest impact on speeding up code depends on knowing where the code spends most of its time
# This cannot be done without performace analysis or profiling 
# You should not think about optimizing your code at first. The first thing to think about is how to get the code to run and
# how to make it readable
# General principles of optimization 
# Design first, then optimize 

system.time()# takes an arbitrary R expression as input and returns the amount of time taken to evaluate the expression 
# Computes the time in seconds that is needed to execute an expression
# If there's an error, gives time until the error occurred 
# Returns an object of class proc_time
# - user time: time charged to the CPU(s) for this expression
# - elapsed time: "wall clock" time 
# Elapsed time may be greater than user time if the CPU spends a lot of time waiting around
# Elapsed time may be smaller than user time if your machine has multiple cores/processors (and is capable of using time)
# However, the basic R program does not use multiples cores as of yet. But, it links to libraries that do use multiple cores
# and the most common one would be the lienar algebra type of library (regression, prediction routines, matrix computations)
# these libraries optimized to use multiple cores and are called multi-threaded BLAS libraries 
# there is also parallel processing libraries for example parallel package which can use multiple cores but can also use multiple computers 

system.time()# allows you to test certain functions or code blocks to see if they are taking excessive amounts of time
# assumes you already know where the problem is and can call system.time() on it
# what if you don't know where to start

# The R Profiler
# The Rprof() starts the profiler in R
# The summaryRprof() function summarizes the output from Rprof() (otherwise it's not readable)
# Do not use system.time() and Rprof() together 
#keeps track of the function call stack at regularly sampled intervals
# and tabulates how much time is spent in each function
# Default sampling interval is 0.02 seconds
# So it prints out the functin call stack at every 0.02 seconds 
# If your function takes less than 0.02 seconds to run, then the Rprofiler will be useless
# because it will never sample the function call stack 

summaryRprof()
# tabulates the R profiler output and calculates how much time is spent in which function 
# There aer two methods for normalizing the data 
# "by.total" divides the time spent in each function by the total run time
# "by.self" does the same but first subtracts out time spent in functions above in the call stack 
# so in a way the "by.self" tells you how much time is spent in a given function, but after subtracting
# all the time spent in lower level helper functions that it calls, so it gives you a more accurat picture of 
# the functions that truly taking up the most amount of time to target for optimization 
# C or Fortran code in not prfiled. You won't see any information about that code. All you will know
# is that some time is pent there, but you won't know any details about that. 
# so the profiler is very usefult in collecting data about where time is being spent

library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)

# Swirl practice
# If you are curious as to how much space the dataset is occupying in memory, you can use object.size(plants).
# names(plants) will return a character vector of column (i.e. variable) names.
# summary() provides different output for each variable, depending on its class. For numeric data such as Precip_Min, summary()
# displays the minimum, 1st quartile, median, mean, 3rd quartile, and maximum. These values help us understand how the data are
# distributed.
# For categorical variables (called 'factor' variables in R), summary() displays the number of times each value (or 'level') occurs in
# the data. For example, each value of Scientific_Name only appears once, since it is unique to a specific plant. In contrast, the
# summary for Duration (also a factor variable) tells us that our dataset contains 3031 Perennial plants, 682 Annual plants, etc.

# When the 'size' argument to sample() is not specified, R takes a sample equal in size to the vector from which you are sampling.

# we can use rbinom() to simulate a binomial random variable
# A binomial random variable represents the number of 'successes' (heads) in a given number of independent 'trials' (coin flips).
sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
rbinom(1, size = 100, prob = 0.7)# Note that you only specify the probability of 'success' (heads) and NOT the probability of 'failure' (tails).
rbinom(100, size = 1, prob = 0.7)# if we want to see all of the 0s and 1s

# to simulate 100 *groups* of random numbers, each containing 5 values generated from a Poisson distribution with mean 10
replicate(100, rpois(5, 10))
# replicate() created a matrix, each column of which contains 5 random numbers generated from a Poisson distribution with mean 10.
cm <- colMeans(my_pois)
hist(cm)# to look at the distribution of our column means 

# Each probability distribution in R has an r*** function (for "random"), a d*** function (for "density"), a p*** (for "probability"), and
# q*** (for "quantile").
# All of the standard probability distributions are built into R, including exponential (rexp()), chi-squared (rchisq()), gamma (rgamma()). 

# Before plotting, it is always a good idea to get a sense of the data. Key R commands for doing so include, dim(), names(),
# head(), tail() and summary().

data(cars)
plot(cars)# First, R notes that the data frame you have given it has just two columns, so it assumes that you want to 
# plot one column versus the other. Second, since we do not provide labels for either axis, R uses the names of the columns.
# Third, it creates axis tick marks at nice round numbers and labels them accordingly. Fourth, it uses the other defaults supplied in plot().
# Note that 'plot' is short for scatterplot.
plot(x = cars$speed, y = cars$dist)
plot(dist ~ speed, cars)#"formula" interface

# Recreate the plot with the label of the x-axis set to "Speed".
plot(x = cars$speed, y = cars$dist, xlab = "Speed")
# Recreate the plot with the label of the y-axis set to "Stopping Distance".
plot(x = cars$speed, y = cars$dist, ylab = "Stopping Distance")
# Recreate the plot with "Speed" and "Stopping Distance" as axis labels.
plot(x = cars$speed, y = cars$dist, xlab = "Speed", ylab = "Stopping Distance")
# Plot cars with a main title of "My Plot". Note that the argument for the main title is "main" not "title".
plot(cars, main = "My Plot")
# Plot cars with a sub title of "My Plot Subtitle".
plot(cars, sub = "My Plot Subtitle")

# The plot help page (?plot) only covers a small number of the many arguments that can be passed in to plot() and to other
# graphical functions. To begin to explore the many other options, look at ?par.

# Plot cars so that the plotted points are colored red. (Use col = 2 to achieve this effect.)
plot(cars, col = 2)

# Plot cars while limiting the x-axis to 10 through 15.  (Use xlim = c(10, 15) to achieve this effect.)
plot(cars, xlim = c(10, 15))

# You can also change the shape of the symbols in the plot. The help page for points (?points) provides the details.
# Plot cars using triangles.  (Use pch = 2 to achieve this effect.)
plot(cars, pch = 2)

# Arguments like "col" and "pch" may not seem very intuitive. And that is because they aren't! So, many/most people use more
# modern packages, like ggplot2, for creating their graphics in R.

# to load the mtcars data
data(mtcars)

# Instead of adding data columns directly as input arguments, as we did with plot(), it is often handy to pass in the entire data
# frame. This is what the "data" argument in boxplot() allows.
# boxplot(), like many R functions, also takes a "formula" argument, generally an expression with a tilde ("~") which indicates the
# relationship between the input variables. This allows you to enter something like mpg ~ cyl to plot the relationship between cyl
# (number of cylinders) on the x-axis and mpg (miles per gallon) on the y-axis.
boxplot(formula = mpg ~ cyl, data = mtcars)
# Note that we can use the same set of arguments that we explored with plot() above to add axis labels, titles and so on.

# When looking at a single variable, histograms are a useful tool. hist() is the associated R function. Like plot(), hist() is best
# used by just passing in a single vector.
hist(mtcars$mpg)

### Getting and cleaning data
# The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set.
# The goal is to prepare tidy data that can be used for later analysis.

# The raw data may only need to be processed once, but regardless of how often you process it, 
# you need to keep a record of all the different things you did. 
# Because it can have a major impact on the data stream analysis. 
# The processed data is data that is ready for analysis.

# So the processing of the data might include merging, subsetting, transforming, 
# or you might go into a file and extract out a part of an image. 
# You might go into a file and extract out a little bit of text from a preformed text field.
# A very critical component of data analysis is that all steps should be recorded. 

# The four things that you should have going from a raw data set into a tidy one
# 1. The raw data

# 2. A tidy data set
## - Each variable you measure should be in one column
## - Each different observation of that variable should be in a different row
## - There should be one table for each "kind" of variable 
## - If you have multiple tables, they should include a column in the table that alows them to be linked 
### General tips: include a row at the top of each file with variable names, make variable names human readable 
### such as AgeAtDiagnosis instead of AgeDx, data should be saved in one file per table 

# 3. A code book describing each variable and its values (this code book is called the metadata
# so it is the data that surrounds the data and explains what the data is trying to say)
## 1. Information about the variables (including units!) 
## 2. Information about summary choices (Mean vs Median)
## 3. Information about the experimental study design (The way that you collected data, whether it was just in a database 
## and was extracted out or whether you performed an experiment. 
# A common format is a Word or text file, like Markdown files. 
# There should be a section called "Study design" that has a thorough description of how you collected the data.
# (so this should say how you picked which observations to collect, what did you extract out of the database,
# what did you exclude...)
# There should also be a section called "Code book" that describes each variables and its units 

# 4. An explicit and exact recipe you used to go from 1 -> 2,3 (R scripts) (The input is the raw data and the output is the processed
# data)

# Downloading files
# It is better to include the downloading process in the processing script to get a more complete picture of how the data 
# were collected 
# A basic component of working with data is knowing your working directory
# The two main commands:
getwd()
setwd()
# Be aware of relative versus absolute paths:
# - Relative - setwd("./data"), 
# - Relative - setwd("../") --> you'll move it up one directory? 
# - Absolute - setwd("/Users/jtleek/data/")
 
# Checking for and creating directories
file.exists("directoryName")# will check to see if the directory exists
dir.create("directoryName")# will create a directory if it doesn't exist

if (!file.exists("data")) {
    dir.create("data")
}

# Getting data from the internet
download.file()
# Important parameters are url, destfile (is the destination file, where that data is going to go), method 
# Useful for downloading tab-limited, csv, and other files, it is agnostic to the file type 
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
dir.create("data")
getwd()# "/Users/maryamghaedi"
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")# we need to use curl method becuase the website is https, a secure website
# - if the url starts with http you can use download.file()
# - if the url starts with https you may need to set method = "curl

list.files("./data")

dateDownloaded <- date()# An important component of downloading a file from internet is that 
# those files might change and you want to keep track of the date that you downloaded the data 
dateDownloaded

# Reading local flat files (text files, tab delimited text files, comma delimited text files...)
# The most common function for reading data into R:
read.table()
# reads data into RAM - big data can cause problem so probably not the best way to read large datasets in R 
# Important parameters are file, header (variable names at the top of each column), sep, row.names, nrows (how many rows you want to read)
# The default for read.table is to read tab delimited files so if reading csv, sep = ","
# quote - you can tell R whether there are any quoted values, quote = "" means no quote
# na.strings - set the character that represents a missing value 
# nrows - how many rows to read of the file (nrows = 10 reads 10 lines)
# skip - number of lines to skip before starting to read, nrows = 10, skip 2 will read 3 - 13 
# The biggest trouble with reading flat files are quotation marks ' or " placed in data values, setting quote = "" often resolves this problem
# Whenever you see datasets that are not getting read in or read in as long vector setting the quote variable can be one way to address it

# Related functions are read.csv() and read.csv2()

cameraData <- read.table("./data/cameras.csv", sep = ",", header = TRUE)
head(cameraData)

read.csv()#automatically sets sep = "," and header = TRUE

# Reading Excel files
library(xlsx)# the library tht is useful for reading excel file is xlsx package

fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD&bom=true&format=true"
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")# I saved the csv file as an xlsx file after the download
list.files("data")
dateDownloaded <- date()

cameraData <- read.xlsx("./data/cameras.xlsx", sheetIndex = 1, header = TRUE)# here I opened the xlsx file
# the sheetIndex is which sheet to store data on 

# To read specific roads and columns:
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx", sheetIndex = 1, colIndex = colIndex, rowIndex = rowIndex)
cameraDataSubset

write.xlsx()# will write out an Excel file with similar arguments
read.xlsx2()# is much faster than read.xlsx but for reading subsets of rows may be slightly unstable 
# The XLConnect package has more options for writing and manipulating Excel files
# The XLConnect vignette is a good place to start for the package 
# In general it is advised to store your data in either a database or in comma separated files
# (.csv) or tab separated files (.tab/.txt) as they are easier to distribute 

## Reading XML (Extensible markup language)
# Widely used in internet applications so you'll see it a lot when you're doing web scraping or trying to get data
# an internet API or trying to download data from open data of websites, like an open government website 
# There are two components to an XML file: 
# - Markup - labels that give the text structure 
# - Content - the actual text of the document that is typed in between the labels 

# Tags, elements and attributes 
# Tags correspond to general labels:
# - Start tags <section>
# - End tags </section>
# - Empty tags <line-break />

# Elements are specific examples are tags
# - <Greeting> Hello, world </Greeting>

# Attributes are components of the label so you can actually add to the tags components
# - <img src="jef.jpg" alt="instructor"/>
# - <step number="3"> Connect A to B. </step>

library(XML)
library(RCurl)
# Attention: xmlTreeParse does not support https. You can load the data with getURL (from RCurl) and then parse it. 
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
doc0 <- getURL(fileUrl)#this loads the document into R memory in a way that you can then parse it and 
doc0
class(doc0)
doc <- xmlParse(doc0)
doc
rootNode <- xmlRoot(doc)# you can get the root Node with xmlRoot; it is the wrapper element for the entire document 
xmlName(rootNode)# To get the name of the root Node
names(rootNode)# what all the nested elements within that root node are. Here there are five different breakfast items on the menu
# and each one is wrapped within a food element. 
# so within R, it is still object, so we have to be able to use different functions to access different parts of that object.

rootNode[[1]]
rootNode[[1]][[1]]
# So in the same way you access list in R

# To programmatically extract different parts of the file. 
# you pass xmlSApply a parsed XML object and then you tell it what function you like to apply; so in this case XML value. 
# so it's going to loop through all of the elements of the root node and get the XML value And by default this is going to do this recursively. 
# since rootNode contains the entire document, it's going to go through and get every single value of every single tagged element in the entire document. 
# and so you just get a bunch of text all strung together that's all the text that was in that document.
xmlSApply(rootNode, xmlValue)

# You might be a little bit more specific and get a specific component of the document.
# And you can do that using the XPath language.
# /node Top level node
# //node Node at any level
# node[@attr-name] Node with an attribute name
# node[@attr-name='bob'] Node with attribute name att-name='bob'
xpathSApply(rootNode, "//name", xmlValue)#This is going to get all the nodes that correspond to an element with title name
# and then it is going to check the xml values of those nodes
# so it is going to take all of the elements of the XML file that are tagged with the name 

xpathSApply(rootNode, "//price", xmlValue)
doc
xpathSApply(doc, "//price", xmlValue)

fileUrl <- "https://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"# I copied this from the link opened on my computer
doc0 <- getURL(fileUrl)
doc0
class(doc0)
doc <- htmlTreeParse(doc0, useInternalNodes = TRUE)
doc
class(doc)

teams <- xpathSApply(doc, "//div[@class='game-info']", xmlValue)
teams

# Reading JSON (Javascript Object Notation)
# Similar to XML in a sense that it is structured and is also very commonly used on the internet 
# Lightweight data storage
# Common format for data from application programming interfaces (APIs), which are ways that you can programmatically get access
# to data for companies like Twitter or Facebook through URLs
# Similar structure to XML but different syntax/format
# Data stored as: Numbers (double), Strings (double quoted), Boolean (true or false), Array (ordered, comma separated 
# enclosed in square brackets[]), Object (unordered, comma separated collection of key:value pairs in curly brackets {})

https://api.github.com/users/jtleek/repos
# There is an overal square bracket that represents the entire JSON object.
# Each of the different repos is inside of its own separate curly bracket. Each repo has a bunch of variables associated with it.
# Such as "id" and "full_name" here. The "owner" variable here has an array as value
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")# what you get out is a structured data frame
jsonData
names(jsonData)# all the top level variables 
names(jsonData$owner)# one of the variables in owner and to see the names within this particular variable  
# accessing one column of the data frame, which is one its own a data frame (so in a way a data frame is stored within a
# data frame)
jsonData$owner$login

# Writing data frames to JSON
myjson <- toJSON(iris, pretty = TRUE)# you can take data frames in R and turn it into JSON dataset 
# This is nice if you are going to be exporing data that's gonna be used by API that requires JSON formatted data
# pretty = TRUE gives a nice indentation and it will be easy to read file when you look at it 
cat(myjson)# to print it out

# Convert back to JSON
iris2 <- fromJSON(myjson)# send it back to a data frame 
head(iris2)

# data.table package (which is often faster and more memory efficient version of data frames)
# data.table inherits from a data frame so all functions that accept data.frame should work on data.table. 
# It's written in C so it can be much, much faster than some of the functions that are done in data.frame. 
# (It's much, much faster at subsetting, grouping variables, and updating variables than data frames are 
# but it requires you to learn a little bit of a new syntax so there's a little bit of a learning curve.)

library(data.table)
DF = data.frame(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))
DF
head(DF, 3)

DT = data.table(x = rnorm(9), y = rep(c("a", "b", "c"), each = 3), z = rnorm(9))
DT
head(DT, 3)

tables()# to see all of the data tables in the memory, for each data table, it will tell you the name of the DT, numbr of rows,
# how many mega bites, how many columns and if there is a key

DT[2,]
DT[DT$y=="a",]
DT[c(2,3)]# you can subset with only one index and it subsets based on the rows (so this is different than with a data frame?)
DT[,c(2,3)]# In the course it says it doesn't work but it does for me

# Column subsetting in data.table (the use of expressions to summarize data in various different ways)
# The subsetting function is modified for data.table
# The argument you pass after the comma is called an "expression"
# In R an expression is a collection of statements enclosed in curly brackets

{
    x = 1
    y = 2
}

k = {print(10); 5}#10
print(k)#5

# Instead of putting an index in the second part of the brackets, you can pass a list of functions 
# that you want to perform where the functions are applied to variables named by columns. 
# And note that we don't have to use quotation marks.
# And so, anytime you pass a list into this second argument, it'll perform those functions and return the values. 
DT[,list(mean(x), sum(z))]
DT[,table(y)]# To get a table of the y values.

# Another thing that it does very fast and memory efficiently is to add a new column. 
# So suppose you wanted to add a new column to your data table where a new column was equal to this z variable squared 
# to add new column use :=
# And the nice thing is is usually when you're adding a new variable to a data frame, R will copy over the entire data frame 
# and add a new variable to it, so you get two copies of the the data frame in memory. 
# when dealing with big data sets, this is obviously going to cause lots of memory problem
# with data table a new copy isn't being created
DT[,w := z^2]# This is to add a new column so the := adds the variable w which is z^2 as a column to the data table 
DT

# You have to be careful though
DT2 <- DT
DT[, y:=2]
head(DT, n = 3)
head(DT2, n = 3)
# Although we only changed the DT, the DT2 is also changed, becuase a copy hasn't been made 
# If you are trying to create a copy, you have to explicitly do that with the copy function 

DT[,m:={tmp <- (x+z); log2(tmp+5)}]# you can perfrom multiple step functions to create new variables 
# here the expression starts with the curly bracket and ends with the curly bracket 
# and each statement is followed by a semi-colon
# so the first statement, I will assign to the temporary variable the values of x+z
# and then I will take a log base 2 of that temporary variable plus 5 
# the last thing that is returned from this expression is the evaluation of the last statement 
# so the varialbe m will be assigned to be log base two of x + z + 5
DT

DT[,a:=x>0]
DT

# So suppose we want to summarize another variable by the case s where when x is greater than zero versus the cases when x is less than zero
# so for example we can take the mean of z + w, and we can do it grouped by a variable A 
# so it is going to take the mean of x + w, when a is equal to TRUE 
# and it's going to place that mean in all the rows of b where a is equal to TRUE
# then it is going to take th emean of x + w, where a is equal to FALSE 
# and place that mean in all the rows of b where a is equal to FALSE 
DT[,b:=mean(z+w), by=a]
DT

# Special Variables in data table that allow to do things very fast
# .N An integer, length 1, containing the number of times that a particular group appears
set.seed(123)
DT <- data.table(x = sample(letters[1:3], 1E5, TRUE))# 1E5 is 100,000
# to count the number of times each of those letters appear
DT[, .N, by=x]#count the number of times grouped by the variable x 

# A unique aspect of data tables is that they have keys. If you set a key, it is possible to subset and sort 
# a data table much more rapidly than you would be able to do with data frame
DT <- data.table(x = rep(c("a", "b", "c"), each = 100), y = rnorm(300))
setkey(DT, x)# the key for this data table is the variable x 
DT['a']# so when I put quoted a here, it knows to go and look in the key which is x
# and very quickly subsets the data to only the values of x that are equal to a 

# You can also use keys to facilitate joints between data tables
DT1 <- data.table(x = c('a', 'a', 'b', 'dt1'), y = 1:4)
DT1
DT2 <- data.table(x = c('a', 'b', 'dt2'), z = 5:7)
DT2
setkey(DT1, x); setkey(DT2, x)# the same key for both data tables 
merge(DT1, DT2)

# It can also be advantagous to use data tables if you want to read things fast 
big_df <- data.frame(x = rnorm(1E6), y = rnorm(1E6))
file <- tempfile()# to set up a temporary file 
write.table(big_df, file = file, row.names = F, col.names = T, sep = "\t", quote = F)# to write the big data frame out to that file
system.time(fread(file))# elapsed 0.087, the fread command can be applied to reading data tables 
system.time(read.table(file, header = T, sep = "\t"))# elapsed 4.311 

# swirl, Getting and Cleaning Data
# dplyr is a fast and powerful R package written by Hadley Wickham and Romain Francois 
# that provides a consistent and concise grammar for manipulating tabular data.

# One unique aspect of dplyr is that the same set of tools allow you to work with tabular data from a variety of sources,
# including data frames, data tables, databases and multidimensional arrays. 

# "CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions
# of code and documentation for R" (http://cran.rstudio.com/)

library(dplyr)
packageVersion("dplyr")

# The first step of working with data in dplyr is to load the data into what the package authors call a 'data frame tbl'
# or 'tbl_df'. 
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
cran <- tbl_df(mydf)
# From ?tbl_df, "The main advantage to using a tbl_df over a regular data frame is the printing.
# First, we are shown the class and dimensions of the dataset. Just below that, we get a preview of the data. Instead of
# attempting to print the entire dataset, dplyr just shows us the first 10 rows of data and only as many columns as fit
# neatly in our console. At the bottom, we see the names and classes for any variables that didn't fit on our screen.

# "The dplyr philosophy is to have small functions that each do one thing well." 
# Specifically, dplyr supplies five 'verbs' that cover most fundamental data manipulation tasks:
# select(), filter(), arrange(), mutate(), and summarize().

# As may often be the case, particularly with larger datasets, we are only interested in some of the variables. 
# Use select(cran, ip_id, package, country) to select only the ip_id, package, and country variables from the cran dataset.
select(cran, ip_id, package, country)
# so we don't have to type cran$ip_id, cran$package, and cran$country as we normally would when
# referring to columns of a data frame. The select() function knows we are referring to columns of the cran dataset.
# Also, note that the columns are returned to us in the order we specified, even though ip_id is the rightmost column in the original
# dataset.

# Normally, the `:` operator is reserved for numbers, but select() allows you to specify a sequence of columns this way, which can save a
# bunch of typing. Use select(cran, r_arch:country) to select all columns starting from r_arch and ending with country.
select(cran, r_arch:country)
# We can also select the same columns in reverse order.
select(cran, country:r_arch)

# Instead of specifying the columns we want to keep, we can also specify the columns we want to throw away. To see how this works, do
# select(cran, -time) to omit the time column.
select(cran, -time)
select(cran, -(X:size))# to omit all columns from X through size

# filter function to select a subset of rows 
# Use filter(cran, package == "swirl") to select all rows for which the package variable is equal to "swirl".
# You can specify as many conditions as you want, separated by commas. For example filter(cran, r_version == "3.1.1", country == "US") will
# return all rows of cran corresponding to downloads from users in the US running R version 3.1.1. Try it out.
# The conditions passed to filter() can make use of any of the standard comparison operators. Pull up the relevant documentation with
# ?Comparison (that's an uppercase C).
filter(cran, country == "IN", r_version <= "3.0.2")

# We can also request rows for which EITHER one condition OR another condition are TRUE. 
# For example, filter(cran, country == "US" | country == "IN") will gives us all
# rows for which the country variable equals either "US" or "IN". 

filter(cran, size > 100500, r_os == "linux-gnu")
# will give us all rows for which size is strictly greater than 100500 and r_os is "linux-gnu".

# Finally, we want to get only the rows for which the r_version is not missing. R represents missing values with NA and these missing values
# can be detected using the is.na() function.
filter(cran, !is.na(r_version))

# Inherent in select() was also the ability to arrange our selected columns in any order we please.

# Sometimes we want to order the rows of a dataset according to the values of a particular variable. This is the job of arrange().
arrange(cran2, ip_id)# to order the ROWS of cran2 so that ip_id is in ascending order (from small to large)
# To do the same, but in descending order, change the second argument to desc(ip_id), where desc() stands for 'descending'.
arrange(cran2, desc(ip_id))

# We can also arrange the data according to the values of multiple variables. For example, arrange(cran2, package, ip_id) will first arrange
# by package names (ascending alphabetically), then by ip_id. This means that if there are multiple rows with the same value for package, they
# will be sorted by ip_id (ascending numerically). 

# It's common to create a new variable based on the value of one or more variables already in a dataset. 
# The mutate() function does exactly this.
# To add a column called size_mb that contains the download size in megabytes:
mutate(cran3, size_mb = size / 2^20)
# One very nice feature of mutate() is that you can use the value computed for your second column (size_mb) to create a third column, all in
# the same line of code. To see this in action, repeat the exact same command as above, except add a third argument creating a column that is
# named size_gb and equal to size_mb / 2^10.
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)

# The last of the five core dplyr verbs, summarize(), collapses the dataset to a single row. 
summarize(cran, avg_bytes = mean(size))
# summarize() is most useful when working with data that has been grouped by the values of a particular variable.
# so summarize() can give you the requested value FOR EACH group in your dataset.

# summarize(), is most powerful when applied to grouped data
# The main idea behind grouping data is that you want to break up your dataset into groups of rows based on the values of one or more
# variables. The group_by() function is reponsible for doing this.
by_package <- group_by(cran, package)
# At the top of the output above, you'll see 'Groups: package', which tells us that this tbl has been grouped by the package variable.
# Everything else looks the same, but now any operation we apply to the grouped data will take place on a per package basis.

# Recall that when we applied mean(size) to the original tbl_df via summarize(), it returned a single number -- the mean of all values in the
# size column. We may care about what that number is, but wouldn't it be so much more interesting to look at the mean download size for each
# unique package?
# That's exactly what you'll get if you use summarize() to apply mean(size) to the grouped data in by_package.
summarize(by_package, mean(size))
# Instead of returning a single value, summarize() now returns the mean size for EACH package in our dataset.

# Compute four values, in the following order, from
# the grouped data:
#
# 1. count = n()
# 2. unique = n_distinct(ip_id)
# 3. countries = n_distinct(country)
# 4. avg_bytes = mean(size)

pack_sum <- summarize(by_package,
                      count = n(),
                      unique = n_distinct(ip_id),
                      countries = n_distinct(country),
                      avg_bytes = mean(size))

# The 'count' column, created with n(), contains the total number of rows (i.e. downloads) for each package. The 'unique' column, created
# with n_distinct(ip_id), gives the total number of unique downloads for each package, as measured by the number of distinct ip_id's. The
# 'countries' column, created with n_distinct(country), provides the number of countries in which each package was downloaded. And finally,
# the 'avg_bytes' column, created with mean(size), contains the mean download size (in bytes) for each package.

# Naturally, we'd like to know which packages were most popular on the day these data were collected (July 8, 2014). Let's start by isolating
# the top 1% of packages, based on the total number of downloads as measured by the 'count' column.
# We need to know the value of 'count' that splits the data into the top 1% and bottom 99% of packages based on total downloads. In
# statistics, this is called the 0.99, or 99%, sample quantile. Use quantile(pack_sum$count, probs = 0.99) to determine this number.
quantile(pack_sum$count, probs = 0.99)
# 99%
# 679.56 

# Now we can isolate only those packages which had more than 679 total downloads. Use filter() to select all rows from pack_sum for which
# 'count' is strictly greater (>) than 679. Store the result in a new object called top_counts.
top_counts <- filter(pack_sum, count > 679)
# There are only 61 packages in our top 1%, so we'd like to see all of them. Since dplyr only shows us the first 10 rows, we can use the
# View() function to see more.
View(top_counts)

# arrange() the rows of top_counts based on the 'count' column and assign the result to a new object called top_counts_sorted. We want the
# packages with the highest number of downloads at the top, which means we want 'count' to be in descending order.
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)

# Perhaps we're more interested in the number of *unique* downloads on this particular day. In other words, if a package is downloaded ten
# times in one day from the same computer, we may wish to count that as only one download. That's what the 'unique' column will tell us.
# Like we did with 'count', let's find the 0.99, or 99%, quantile for the 'unique' variable with quantile(pack_sum$unique, probs = 0.99).
quantile(pack_sum$unique, probs = 0.99)
# 99% 
# 465
# Apply filter() to pack_sum to select all rows corresponding to values of 'unique' that are strictly greater than 465. Assign the result to
# a object called top_unique.
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)

# Our final metric of popularity is the number of distinct countries from which each package was downloaded. We'll approach this one a little
# differently to introduce you to a method called 'chaining' (or 'piping').
# Chaining allows you to string together multiple function calls in a way that is compact and readable, while still accomplishing the desired
# result. To make it more concrete, let's compute our last popularity metric from scratch, starting with our original data.
# We've already done this part, but we're repeating it
# here for clarity.

by_package <- group_by(cran, package)
pack_sum <- summarize(by_package,
                      count = n(),
                      unique = n_distinct(ip_id),
                      countries = n_distinct(country),
                      avg_bytes = mean(size))

# Here's the new bit, but using the same approach we've
# been using this whole time.

top_countries <- filter(pack_sum, countries > 60)
result1 <- arrange(top_countries, desc(countries), avg_bytes)

# Print the results to the console.
print(result1)
# It's worth noting that we sorted primarily by country, but used avg_bytes (in ascending order) as a tie breaker. This means that if two packages
# were downloaded from the same number of countries, the package with a smaller average download size received a higher ranking.

# We'd like to accomplish the same result as the last script, but avoid saving our intermediate results. This requires embedding function calls
# within one another.
# In this script, we've used a special chaining operator, %>%, which was originally introduced in the magrittr R package and has now become a key
# component of dplyr. You can pull up the related documentation with ?chain. The benefit of %>% is that it allows us to chain the function calls
# in a linear fashion. The code to the right of %>% operates on the result from the code to the left of %>%.
# you can pronounce the %>% operator as the word 'then':
result3 <-
    cran %>%
    group_by(package) %>%
    summarize(count = n(),
              unique = n_distinct(ip_id),
              countries = n_distinct(country),
              avg_bytes = mean(size)
    ) %>%
    filter(countries > 60) %>%
    arrange(desc(countries), avg_bytes)

# Print result to console
print(result3)
# this script provides a convenient and concise alternative to the more
# traditional method that we've taken previously, which involves saving results as we go along.
View(result3)

# Let's build a chain of dplyr commands one step at a time:
# select() the following columns from cran. Keep in mind
# that when you're using the chaining operator, you don't
# need to specify the name of the data tbl in your call to
# select().
#
# 1. ip_id
# 2. country
# 3. package
# 4. size
#
# The call to print() at the end of the chain is optional,
# but necessary if you want your results printed to the
# console. Note that since there are no additional arguments
# to print(), you can leave off the parentheses after
# the function name. This is a convenient feature of the %>%
# operator.

cran %>%
    select(ip_id, country, package, size) %>%
    print


# Use mutate() to add a column called size_mb that contains
# the size of each download in megabytes (i.e. size / 2^20).
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20)


## Use filter() to select all rows for which size_mb is
# less than or equal to (<=) 0.5.
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20) %>%
    filter(size_mb <= 0.5)


# arrange() the result by size_mb, in descending order.
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20) %>%
    filter(size_mb <= 0.5) %>%
    arrange(desc(size_mb))

# In this lesson, you'll learn how to tidy your data with the tidyr package.
library(tidyr)
# The author of tidyr, Hadley Wickham, discusses his philosophy of tidy data in his 'Tidy Data' paper:
# http://vita.had.co.nz/papers/tidy-data.pdf

# Tidy data is formatted in a standard way that facilitates exploration and analysis and works seamlessly with other tidy data tools.
# Specifically, tidy data satisfies three conditions:
# Each variable forms a column
# Each observation forms a row
# Each type of observational unit forms a table

# Any dataset that doesn't satisfy these conditions is considered 'messy' data. 
# Therefore, all of the following are characteristics of messy data:
# 1: Column headers are values, not variable names
# 2: Variables are stored in both rows and columns
# 3: A single observational unit is stored in multiple tables
# 4: Multiple types of observational units are stored in the same table
# 5: Multiple variables are stored in one column

# 1: Column headers are values, not variable names
students
#    grade male female
#1     A    1      5
#2     B    5      0
#3     C    5      2
#4     D    5      5
#5     E    7      4
# This dataset actually has three variables: grade, sex, and count. The first variable, grade, is already a column, so that should remain as it
# is. The second variable, sex, is captured by the second and third column headings. The third variable, count, is the number of students for each
# combination of grade and sex.
# To tidy the students data, we need to have one column for each of these three variables. We'll use the gather() function from tidyr to
# accomplish this.
gather(students, sex, count, -grade)# Note the minus sign before grade, which says we want to gather all columns EXCEPT grade.
#     grade    sex count
#1      A   male     1
#2      B   male     5
#3      C   male     5
#4      D   male     5
#5      E   male     7
#6      A female     5
#7      B female     0
#8      C female     2
#9      D female     5
#10     E female     4
# The data argument, students, gives the name of the original dataset.
# The key and value arguments -- sex and count, respectively -- give the column names for our tidy dataset. The final argument, -grade, says
# that we want to gather all columns EXCEPT the grade column (since grade is already a proper column variable.)

# 5: Multiple variables are stored in one column
students2
#   grade male_1 female_1 male_2 female_2
#1     A      3        4      3        4
#2     B      6        4      3        5
#3     C      7        4      3        8
#4     D      4        0      8        1
#5     E      1        1      2        7
# This dataset is similar to the first, except now there are two separate classes, 1 and 2, and we have total counts for each sex within each
# class. students2 suffers from the same messy data problem of having column headers that are values (male_1, female_1, etc.) and not variable
# names (sex, class, and count). However, it also has multiple variables stored in each column (sex and class), which is another common symptom of messy data. Tidying this
# dataset will be a two step process.
res <- gather(students2, sex_class, count, -grade)
res
#    grade sex_class count
#1      A    male_1     3
#2      B    male_1     6
#3      C    male_1     7
#4      D    male_1     4
#5      E    male_1     1
#6      A  female_1     4
#7      B  female_1     4
#8      C  female_1     4
#9      D  female_1     0
#10     E  female_1     1
#11     A    male_2     3
#12     B    male_2     3
#13     C    male_2     3
#14     D    male_2     8
#15     E    male_2     2
#16     A  female_2     4
#17     B  female_2     5
#18     C  female_2     8
#19     D  female_2     1
#20     E  female_2     7

# That got us half way to tidy data, but we still have two different variables, sex and class, stored together in the sex_class column. tidyr
# offers a convenient separate() function for the purpose of separating one column into multiple columns.
# Call separate() on res to split the sex_class column into sex and class. You only need to specify the first three arguments: data = res, col =
# sex_class, into = c("sex", "class"). 
separate(res, col = sex_class, into = c("sex", "class"))
#    grade sex class count
#1      A   male     1     3
#2      B   male     1     6
#3      C   male     1     7
#4      D   male     1     4
#5      E   male     1     1
#6      A female     1     4
#7      B female     1     4
#8      C female     1     4
#9      D female     1     0
#10     E female     1     1
#11     A   male     2     3
#12     B   male     2     3
#13     C   male     2     3
#14     D   male     2     8
#15     E   male     2     2
#16     A female     2     4
#17     B female     2     5
#18     C female     2     8
#19     D female     2     1
#20     E female     2     7
# Conveniently, separate() was able to figure out on its own how to separate the sex_class column. Unless you request otherwise with the 'sep'
# argument, it splits on non-alphanumeric values. In other words, it assumes that the values are separated by something other than a letter or number
# (in this case, an underscore.)

# Tidying students2 required both gather() and separate(), causing us to save an intermediate result (res). However, just like with dplyr, you can use
# the %>% operator to chain multiple function calls together.

# Repeat your calls to gather() and separate(), but this time
# use the %>% operator to chain the commands together without
# storing an intermediate result.
#
# If this is your first time seeing the %>% operator, check
# out ?chain, which will bring up the relevant documentation.
# You can also look at the Examples section at the bottom
# of ?gather and ?separate.
#
# The main idea is that the result to the left of %>%
# takes the place of the first argument of the function to
# the right. Therefore, you OMIT THE FIRST ARGUMENT to each
# function.

students2 %>%
    gather(sex_class, count, -grade) %>%
    separate(sex_class, c("sex", "class")) %>%
    print

# 2: Variables are stored in both rows and columns
students3
#   name   test class1 class2 class3 class4 class5
# 1 Sally midterm   A   <NA>    B   <NA>     <NA>
# 2 Sally   final   C   <NA>    C   <NA>     <NA>
# 3 Jeff midterm   <NA>  D     <NA>   A      <NA>
# 4 Jeff   final   <NA>  E     <NA>   C      <NA>
# 5 Roger midterm  <NA>  C     <NA>  <NA>      B
# 6 Roger   final  <NA>  A     <NA>  <NA>      A
# 7 Karen midterm  <NA>  <NA>    C     A     <NA>
# 8 Karen   final  <NA>  <NA>    C     A     <NA>
# 9 Brian midterm   B   <NA>   <NA>   <NA>      A
#10 Brian   final   B   <NA>   <NA>   <NA>      C
# In students3, we have midterm and final exam grades for five students, each of whom were enrolled in exactly two of five possible classes.
# The first variable, name, is already a column and should remain as it is. The headers of the last five columns, class1 through class5, are all
# different values of what should be a class variable. The values in the test column, midterm and final, should each be its own variable containing
# the respective grades for each student.

# This will require multiple steps, which we will build up gradually using %>%.
# Call gather() to gather the columns class1
# through class5 into a new variable called class.
# The 'key' should be class, and the 'value'
# should be grade.
#
# tidyr makes it easy to reference multiple adjacent
# columns with class1:class5, just like with sequences
# of numbers.
#
# Since each student is only enrolled in two of
# the five possible classes, there are lots of missing
# values (i.e. NAs). Use the argument na.rm = TRUE
# to omit these values from the final result.
#
# Remember that when you're using the %>% operator,
# the value to the left of it gets inserted as the
# first argument to the function on the right.
students3 %>%
    gather(class, grade, class1:class5, na.rm = TRUE) %>%
    print
#name    test  class grade
#1  Sally midterm class1     A
#2  Sally   final class1     C
#9  Brian midterm class1     B
#10 Brian   final class1     B
#13  Jeff midterm class2     D
#14  Jeff   final class2     E
#15 Roger midterm class2     C
#16 Roger   final class2     A
#21 Sally midterm class3     B
#22 Sally   final class3     C
#27 Karen midterm class3     C
#28 Karen   final class3     C
#33  Jeff midterm class4     A
#34  Jeff   final class4     C
#37 Karen midterm class4     A
#38 Karen   final class4     A
#45 Roger midterm class5     B
#46 Roger   final class5     A
#49 Brian midterm class5     A
#50 Brian   final class5     C

# This script builds on the previous one by appending
# a call to spread(), which will allow us to turn the
# values of the test column, midterm and final, into
# column headers (i.e. variables).
#
# You only need to specify two arguments to spread().
# Can you figure out what they are? (Hint: You don't
# have to specify the data argument since we're using
# the %>% operator.
#
students3 %>%
    gather(class, grade, class1:class5, na.rm = TRUE) %>%
    spread(test, grade) %>%
    print

#    name  class final midterm
#1  Brian class1     B       B
#2  Brian class5     C       A
#3   Jeff class2     E       D
#4   Jeff class4     C       A
#5  Karen class3     C       C
#6  Karen class4     A       A
#7  Roger class2     A       C
#8  Roger class5     A       B
#9  Sally class1     C       A
#10 Sally class3     C       B

# readr is required for certain data manipulations, such as `parse_number(), which will be used in the next question.  Let's, (re)load the
# package with library(readr).
library(readr)
# Lastly, we want the values in the class column to simply be 1, 2, ..., 5 and not class1, class2, ..., class5. We can use the parse_number()
# function from readr to accomplish this. To see how it works, try parse_number("class5").
parse_number("class5")#5

# We want the values in the class columns to be
# 1, 2, ..., 5 and not class1, class2, ..., class5.
#
# Use the mutate() function from dplyr along with
# parse_number(). Hint: You can "overwrite" a column
# with mutate() by assigning a new value to the existing
# column instead of creating a new column.
#
# Check out ?mutate and/or ?parse_number if you need
# a refresher.
#
students3 %>%
    gather(class, grade, class1:class5, na.rm = TRUE) %>%
    spread(test, grade) %>%
    mutate(class = parse_number(class)) %>%
    print

# 4: Multiple types of observational units are stored in the same table
students4
#    id  name sex class midterm final
#1  168 Brian   F     1       B     B
#2  168 Brian   F     5       A     C
#3  588 Sally   M     1       A     C
#4  588 Sally   M     3       B     C
#5  710  Jeff   M     2       D     E
#6  710  Jeff   M     4       A     C
#7  731 Roger   F     2       C     A
#8  731 Roger   F     5       B     A
#9  908 Karen   M     3       C     C
#10 908 Karen   M     4       A     A
# students4 is almost the same as our tidy version of students3. The only difference is that students4 provides a unique
# id for each student, as well as his or her sex (M = male; F = female).
# At first glance, there doesn't seem to be much of a problem with students4. All columns are variables and all rows are
# observations. However, notice that each id, name, and sex is repeated twice, which seems quite redundant. This is a
# hint that our data contains multiple observational units in a single table.

# Complete the chained command below so that we are
# selecting the id, name, and sex column from students4
# and storing the result in student_info.
#
student_info <- students4 %>%
    select(id,name, sex) %>%
    print
# Notice anything strange about student_info? It contains five duplicate rows! See the script for directions on how to
# fix this. Save the script and type submit() when you are ready, or type reset() to reset the script to its original
# state.
# Add a call to unique() below, which will remove
# duplicate rows from student_info.
#
# Like with the call to the print() function below,
# you can omit the parentheses after the function name.
# This is a nice feature of %>% that applies when
# there are no additional arguments to specify.
#
student_info <- students4 %>%
    select(id, name, sex) %>%
    unique %>%
    print

# select() the id, class, midterm, and final columns
# (in that order) and store the result in gradebook.
#
gradebook <- students4 %>%
    select(id, class, midterm, final) %>%
    print
# It's important to note that we left the id column in both tables. In the world of relational databases, 'id' is called
# our 'primary key' since it allows us to connect each student listed in student_info with their grades listed in
# gradebook. Without a unique identifier, we might not know how the tables are related. (In this case, we could have also
# used the name variable, since each student happens to have a unique name.)

# 3: A single observational unit is stored in multiple tables (It's the opposite of the fourth problem)
# To illustrate this, we've created two datasets, passed and failed. Take a look at passed now.
passed
#  name class final
#1 Brian     1     B
#2 Roger     2     A
#3 Roger     5     A
#4 Karen     4     A

failed
#   name class final
#1 Brian     5     C
#2 Sally     1     C
#3 Sally     3     C
#4  Jeff     2     E
#5  Jeff     4     C
#6 Karen     3     C
# As you may have inferred from the data, students passed a class if they received a final exam grade of A or
# B and failed otherwise.
# The name of each dataset actually represents the value of a new variable that we will call 'status'. Before joining the
# two tables together, we'll add a new column to each containing this information so that it's not lost when we put
# everything together.
# Use dplyr's mutate() to add a new column to the passed table. The column should be called status and the value,
# "passed" (a character string), should be the same for all students. 'Overwrite' the current version of passed with the
# new one.
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
# Now, pass as arguments the passed and failed tables (in order) to the dplyr function bind_rows(), which will join them
# together into a single unit. Check ?bind_rows if you need help.
bind_rows(passed, failed)
#   name class final status
#1  Brian     1     B passed
#2  Roger     2     A passed
#3  Roger     5     A passed
#4  Karen     4     A passed
#5  Brian     5     C failed
#6  Sally     1     C failed
#7  Sally     3     C failed
#8   Jeff     2     E failed
#9   Jeff     4     C failed
#10 Karen     3     C failed
# Of course, we could arrange the rows however we wish at this point, but the important thing is that each row is an
# observation, each column is a variable, and the table contains a single observational unit. Thus, the data are tidy.

# Dates and Times with lubridate
# In this lesson, we'll explore the lubridate R package, by Garrett Grolemund and Hadley Wickham. According to the package authors,
# "lubridate has a consistent, memorable syntax, that makes working with dates fun instead of frustrating.
library(lubridate)
help(package = lubridate)# to bring up an overview of the package, including the package DESCRIPTION, a list of available functions,
# and a link to the official package vignette.
today()# returns today's date
# There are three components to this date. In order, they are year, month, and day. We can extract any of these components using the
# year(), month(), or day() function, respectively.
year(this_day)
month(this_day)
day(this_day)
wday(this_day)# to get the day of the week. It will be represented as a number, such that 1 =
# Sunday, 2 = Monday, 3 = Tuesday, etc.
wday(this_day, label = TRUE)#to display the *name* of the weekday, (represented as an ordered factor).

# In addition to handling dates, lubridate is great for working with date and time combinations, referred to as date-times. The
# now() function returns the date-time representing this exact moment in time. Give it a try and store the result in a variable
# called this_moment.
now()#"2020-09-03 20:43:15 EDT"
# Just like with dates, we can extract the year, month, day, or day of week. However, we can also use hour(), minute(), and second() to
# extract specific time information.

# lubridate offers a variety of functions for parsing date-times. These functions take the form of ymd(), dmy(), hms(),
# ymd_hms(), etc., where each letter in the name of the function stands for the location of years (y), months (m), days (d), hours (h),
# minutes (m), and/or seconds (s) in the date-time being read in.
my_date <- ymd("1989-05-17")#You must surround the date with quotes.
my_date# "1989-05-17" # It looks almost the same, except for the addition of a time zone
# elow the surface, there's another important change that takes place when lubridate parses a date. 
class(my_date)# "Date"
# So ymd() took a character string as input and returned an object of class POSIXct. It's not necessary that you understand what POSIXct is,
# but just know that it is one way that R stores date-time information internally.

# "1989-05-17" is a fairly standard format, but lubridate is 'smart' enough to figure out many different date-time formats. Use ymd() to
# parse "1989 May 17".
ymd("1989 May 17")
# Despite being formatted differently, the last two dates had the year first, then the month, then the day. Hence, we used ymd() to parse
# them. 
mdy("March 12 1975")

# We can even throw something funky at it and lubridate will often know the right thing to do. Parse 25081985

# We can even throw something funky at it and lubridate will often know the right thing to do. Parse 25081985, which is supposed to
# represent the 25th day of August 1985. Note that we are actually parsing a numeric value here -- not a character string -- so leave off
# the quotes.
dmy(25081985)#"1985-08-25"

ymd("192012")# Warning message: All formats failed to parse. No formats found. 
# repeat the same command, but add two dashes OR two forward slashes to "192012" so that it's clear we want January 2, 1920.
ymd("1920/1/2")# "1920-01-02"

# we can parse date-times:
ymd_hms()  

# Use the appropriate lubridate function to parse "03:22:14" (hh:mm:ss):
hms("03:22:14")

# lubridate is also capable of handling vectors of dates, which is particularly helpful when you need to parse an entire column of data.

# The update() function allows us to update one or more components of a date-time. For example, let's say the current time is 08:34:55
# (hh:mm:ss). Update this_moment to the new time using the following command:
update(this_moment, hours = 8, minutes = 34, seconds = 55)# "2020-09-03 08:34:55 EDT"
# It's important to recognize that the previous command does not alter this_moment unless we reassign the result to this_moment. To see
# this, print the contents of this_moment.
this_moment #"2020-09-03 20:44:36 EDT"
this_moment <- update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment # "2020-09-04 08:34:55 EDT"

# Now, pretend you are in New York City and you are planning to visit a friend in Hong Kong. You seem to have misplaced your itinerary,
# but you know that your flight departs New York at 17:34 (5:34pm) the day after tomorrow. You also know that your flight is scheduled to
# arrive in Hong Kong exactly 15 hours and 50 minutes after departure.
# Let's reconstruct your itinerary from what you can remember, starting with the full date and time of your departure. We will approach
# this by finding the current date in New York, adding 2 full days, then setting the time to 17:34.
# To find the current date in New York, we'll use the now() function again. This time, however, we'll specify the time zone that we want:
# "America/New_York". Store the result in a variable called nyc. Check out ?now if you need help.
nyc <- now("America/New_York")
# For a complete list of valid time zones for use with lubridate, check out the following Wikipedia page:
# http://en.wikipedia.org/wiki/List_of_tz_database_time_zones
# Your flight is the day after tomorrow (in New York time), so we want to add two days to nyc. One nice aspect of lubridate is that it
# allows you to use arithmetic operators on dates and times. In this case, we'd like to add two days to nyc, so we can use the following
# expression: nyc + days(2). Give it a try, storing the result in a variable called depart.
depart <- nyc + days(2)
depart#"2020-09-06 22:02:35 EDT"
# So now depart contains the date of the day after tomorrow. Use update() to add the correct hours (17) and minutes (34) to depart.
# Reassign the result to depart.
depart <- update(depart, hours = 17, minutes = 34)
depart# "2020-09-06 17:34:35 EDT"

# Your friend wants to know what time she should pick you up from the airport in Hong Kong. Now that we have the exact date and time of
# your departure from New York, we can figure out the exact time of your arrival in Hong Kong.
# The first step is to add 15 hours and 50 minutes to your departure time. Recall that nyc + days(2) added two days to the current time in
# New York. Use the same approach to add 15 hours and 50 minutes to the date-time stored in depart. Store the result in a new variable
# called arrive.
arrive <- depart + hours(15) + minutes(50)

# The arrive variable contains the time that it will be in New York when you arrive in Hong Kong. What we really want to know is what time
# it will be in Hong Kong when you arrive, so that your friend knows when to meet you.
# The with_tz() function returns a date-time as it would appear in another time zone.
arrive <- with_tz(arrive, tz = "Asia/Hong_Kong")
arrive

# Fast forward to your arrival in Hong Kong. You and your friend have just met at the airport and you realize that the last time you
# were together was in Singapore on June 17, 2008. Naturally, you'd like to know exactly how long it has been.
# Use the appropriate lubridate function to parse "June 17, 2008", just like you did near the beginning of this lesson. This time,
# however, you should specify an extra argument, tz = "Singapore". Store the result in a variable called last_time.
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time

# Pull up the documentation for interval(), which we'll use to explore how much time has passed between arrive and last_time.
?interval
how_long <- interval(last_time, arrive)
# Now use as.period(how_long) to see how long it's been.
as.period(how_long)
# Because of things like leap years, leap seconds, and daylight savings time, the length of
# any given minute, day, month, week, or year is relative to when it occurs. In contrast, the length of a second is always the same,
# regardless of when it occurs.
# To address these complexities, the authors of lubridate introduce four classes of time related objects: instants, intervals,
# durations, and periods. 

## Reading data from mySQL database
# Free and widely used open source database software 
# Widely used in internet based applications 
# Data are structured in databases, within each database there's a series of tables and within the tables there 
# are a series of fields. So you can think of each table as a dataset, and each field as a column of the dataset. 
# Each row in the database is called a record. 
# Each table (dataset) corresponds to one dataframe in R. And, all of the listed variables are column names in R. 
# A common Id appears in all the tables to link tables

# To install MySQL
http://dev.mysql.com/doc/refman/5.7/en/installing.html

install.packages("RMySQL")
install.packages("DBI")
library(DBI)
library(RMySQL)

# UCSC database
# We are going to be using a web-facing version of a MySQL database, so that we can just show how the R MySQL package works
http://genome.ucsc.edu/goldenPath/help/mysql.html
# What we are going to try to do is access the database and collect some information about a particular genome that we are interested in.

ucscDb <- dbConnect(MySQL(), user = "genome", host = "genome-mysql.cse.ucsc.edu")# to connect to a database
# so this opens a connection and that connection is given this handle: ucscDb
# host is where the MySQL server is   
result <- dbGetQuery(ucscDb, " show databases;"); dbDisconnect(ucscDb);# to apply a query to that database
# this command will go the ucscDb connection and it will run the "show databases" command which is a MySQL command 
# and then we disconnect from the MySQL server and we get a TRUE response from 
# the dbDisconnect meaning that we did in fact disconnet from the server 
result# the result is showing the list of all the databases that are available within the MySQL server in the host address we provided 

# we are going to focus on hg19 database, which is a particular build of the human genome (it's the 19th build of the human genome)
hg19 <- dbConnect(MySQL(), user = "genome", db = "hg19",
                  host = "genome-mysql.cse.ucsc.edu")# so we run the dbConnect again and instead of just passing it the user
# we'll pass it the database with db
# so we want to access the hg19 database within the MySQL server 
# and what we might want to do is to see what are the tables within that database 
# so remember on a server there might be multiple databases and within each database, there will be multiple tables 
# each table corresponding to what you might think of as a data frame 
allTables <- dbListTables(hg19)# to look at the tables that exist in the hg19
length(allTables)#10949 so this may tables in the 
allTables[1:5]# to look at the first 5 tables 
dbListFields(hg19, "affyU133Plus2")# this is one table that we are interested in, and to see all the fields which are column names in this table
dbGetQuery(hg19, "select count(*) from affyU133Plus2")# to know how many rows it has , so we use dbGetQuery with another MySQL command
# to count all the records in the table

affyData <- dbReadTable(hg19, "affyU133Plus2")# to get this one dataframe affyU133Plus2 out of the dataset hg19
head(affyData)# to look at the top part of the dataframe
dim(affyData)# 58463    22

# any particular table might be gigantic and too big to read in R, to select only a subset of data 
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")# select * is to select all the different observations
# from affyU133Plus2 table where the misMatches variable, misMatches is one of the columns in this dataset, is between 1 and 3
# so we sent this query to the database and that is stored remotely in the database 
# but it did not try to suck the data back into your computer
# and if we want to fetch the results of that query we use the fetch command 
affyMis <- fetch(query) ; quantile(affyMis$misMatches)
dim(affyMis)# 500  22

affyMisSmall <- fetch(query, n=10); dbClearResult(query);# this is to suck out very small amount of data, to bring back only the top 10 records
# then we clear out the query
dim(affyMisSmall)# dimension of 10 rows, 10 22
dbDisconnect(hg19)#remember to close the connection immediately after extracting the data that you were interested in 

# Additional Information 
# RMySQL vignete: http://cran.r-project.org/web/packages/RMySQL/RMYSQL.pdf 
# this will give you acess to other MySQL commands that you might need to select data

# List of RMySQL commands: http://www.pantz.org/software/mysql/mysqlcommands.html 
# A nice blog post summarizing other commands: http://www.r-bloggers.com/mysql-and-r/

## Hierarchical data format (HDF5) 
# Used for storing large datasets
# Supports storing a range of data types
# Hierarchical data format

# Groups containing zero or more data sets and metadata (1. Have a group header with group name and list of attributes, 2. Have a group symbol 
# table with a list of objects in group)

# Datasets multidimensional array of data elements with metadata (1. Have a header with name, datatype, dataspace, and storage layout
# 2. Have a data array with the data)

# http://www.hdfgroup.org/HDF5/
# R HDF5 is installed through the bioconductor
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.11")

BiocManager::install("rhdf5")
library(rhdf5)

# In this lecture we are going to create HDF5 files and interact with them.
created <- h5createFile("example.h5")# h5createFile command is used to create HDF5 file set which is example.h5
created

# http://www.bioconductor.org/packages/release/bioc/vignettes/rhdf5/inst/doc/rhdf5.pdf
# https://www.bioconductor.org/packages//2.13/bioc/vignettes/rhdf5/inst/doc/rhdf5.pdf
browseVignettes("rhdf5")

# to create groups within the file
created <-h5createGroup("example.h5", "foo")# to create the group foo
created <-h5createGroup("example.h5", "baa")# to create another group called baa
created <- h5createGroup("example.h5", "foo/foobaa")# to create a subgroup of foo called foobaa
h5ls("example.h5")# like ls file it is listing out all the components of this HDF5 file 

# to write to the specific groups 
A <- matrix(1:10, nr = 5, nc = 2)# we create the matrix A 
h5write(A, "example.h5", "foo/A")# h5write command to write the matrix A to the particular group foo
B <- array(seq(0.1, 2.0, by = 0.1), dim = c(5, 2, 2))# a multidimensional array 
attr(B, "scale") <- "liter" # to add attribute, metadata liter attribute which is the unit attribute 
h5write(B, "example.h5", "foo/foobaa/B")# to write the array to a particular subgroup 
h5ls("example.h5")# now we have all the groups that we created and within those groups we actually have some datasets

# to write a data set directly 
df <- data.frame(1L:5L, seq(0,1, length.out = 5),
                 c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
df
h5write(df, "example.h5", "df")# to write df directly to the top level group 
h5ls("example.h5")

# to read data
readA <- h5read("example.h5", "foo/A")
readB <- h5read("example.h5", "foo/foobaa/B")
readdf <- h5read("example.h5", "df")
readA

# Another advantage of HDF5 file format is that you can easily read and write in chunks
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3, 1))# to write the elements 12, 13 and 14 to the dataset A
# we want to write to the specific parts of A dataset with the indices, write to the first 3 rows in the first column of dataset A 
h5read("example.h5", "foo/A")# you can use a similar index command to read just a sub-component of the dataset as well

# hdf5 can be used to optimize reading and writing from disc in R 

## Reading data from the web
# There are a large number ways that you can read data from the web. 
# This is primarily going to focus on scraping data out of websites and maybe doing a little bit with working with APIs and 
# a little bit about authentication.

# Webscraping: Programatically extracting data from the HTML code of websites or from URLs
# Many websites have information you may want to programmatically read 
# In some cases this is against the terms of service for that website 
# Attempting to read too many pages too quickly can get your IP address blocked 
# https://en.wikipedia.org/wiki/Web_scraping
 
# https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en
con <- url("https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")# to open a connection to the URL with the url function
htmlCode <- readLines(con)# to read data (get data) from the connection, you can have readLines a set number of lines that you like to read
close(con)# Like when working with databases, you want to make sure to close the connection after you have used it
htmlCode# the html code from this connection looks like one big long string of letters, 
# even if you set the readLines to a set number of lines but it will still come out unformatted in this way

# one way to deal with that is to use the XML package
# using the xml package, we can parse the html using the internal nodes to get the complete structure out
library(XML)
library(RCurl)
url <- getURL("https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
html <- htmlTreeParse(url, useInternalNodes = T)
html
xpathSApply(html, "//title", xmlValue)# to get the title of the page

xpathSApply(html, "//td[@class='gsc_a_c']", xmlValue)#found this in the discussion forums,
# my quesion is what is td and how would you know to use gsc_a_c

xpathSApply(html, "//a[@class='gsc_a_ac gs_ibl']", xmlValue)

# GET command from the httr package
# with the httr package you can authenticate yourself for websites
install.packages("httr")
library(httr)
url <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html2 <- GET(url)
html2
content2 <- content(html2, as="text")# to extract the content from the html page as text
parsedHtml <- htmlParse(content2, asText = T)# to parse out the text
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(parsedHtml, "//td[@class='gsc_a_c']", xmlValue)

# Accessing websites with passords
pg1 <- GET("http://httpbin.org/basic-auth/user/passwd")
pg1# Status 401 means I wasn't able to login becuase I haven't been authenticated  
#Response [http://httpbin.org/basic-auth/user/passwd]
#Date: 2021-01-05 18:12
#Status: 401
#Content-Type: <unknown>
#   <EMPTY BODY>

# http://cran.r-project.org/web/packages/httr/httr.pdf

pg2 <- GET("http://httpbin.org/basic-auth/user/passwd",
           authenticate("user", "passwd"))# the authenticate command can be given a username and a password
pg2# Status: 200 means that we were able to get access
#Response [http://httpbin.org/basic-auth/user/passwd]
#Date: 2021-01-05 18:18
#Status: 200
#Content-Type: application/json
#Size: 47 B
#{
#    "authenticated": true, 
#    "user": "user"
#}
names(pg2)

# Using Handles
# By using handles, you can save the authentication across multiple accesses to a website 
google <- handle("http://google.com") # if you authenticate this handle once, the cookies will stay with that handle and 
# you'll be authenticated. So you do not need to keep authenticating over and over again as you access that website. 
pg1 <- GET(handle = google, path="/")
pg1
pg2 <- GET(handle=google, path="search")
pg2

#http://cran.r-project.org/web/packages/httr/httr.pdf

# Notes and further resources 
# R Bloggers has a number of examples of web scraping: http://www.r-bloggers.com/?s=Web+Scraping
# The httr help fule has useful examples: http://cran.r-project.org/web/packages/httr/httr.pdf

## Reading from APIs
# APIs are application programming interfaces
# For example, most internet companies like Twitter or Facebook will have an application programming interface, where you can download data
# For example, you get data on which users are tweeting, or what they're tweeting about 
# You can usually get these with GET request with specefic URLs as the arguments 

# To use httr package to get data from Twitter
# You need to create an account with the API or with the development team of each particular organization 
# Twitter developer account application will get you some numbers to later use to authenticate the application through R and acess data
# click on the application, you can see that you have read and write access to the API and then there are a bunch of numbers 
# consumer key, consumer secret, a request hoping URL and an authorization URL
# these numbers will be used later to access the API

# httr demos on github you can see how to acess different APIs for the different websites 
# https://github.com/r-lib/httr/tree/master/demo


library(httr)
# httr allows GET, POST, PUT, DELETE requests if you are authorized 
# You can authenticate with a user name and a password
# Most modern APIs use somethin like oauth
# httr works well with Facebook, Google, Twitter, Github
myapp <- oauth_app("twitter",# to name the application 
                   key = "yourConsumerKeyHere", secret = "your ConsumerSecretHere")# to start the authorization process for your application 

sig <- sign_oauth1.0(myapp,
                     token = "yourTokenHere",
                     token_secret = "yourTokenSecretHere")# to take the application and sign it

# now I have created the credentials that will allow the access to data that is privately held by Twitter
# and is only available to people with an application 

HomeTL <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)# the URL that corresponds to the Twitter API
# 1.1 is the version of the API 
# then I will pass it some more components which correspond to which data I'd like to get out. Here, we get statuses on my home timeline
# and I am going to get it out as a json file, which is currently the only kind of data supported by Twitter  
# and then I'm going to pass it, instead of authenticating with the username or a password 
# I'm going to pass it the authentication I used with this oauth sign in, sig 
# So what I get back is the page that corresponds to this url which actually is going to be just some json data

# To know which URL to use, go into the documentation for the Twitter API
# https//dev.twitter.com/docs/api/1.1/get/search/tweets
# resource url is to pass to the GET command
# parameters to send to API including how many to collect, what time to collect...

# If you want to get general information, you can go to the main twitter documentation 
# information on mentions, user timelines, home timline, retweets
# https://dev.twitter.com/docs/api/1.1/overview

json1 <- content(homeTL)# to use the content function to extract the json data
# so the content function will recognize that it is json data and return a structured R object that is a little hard to read

json2 <- jsonlite::fromJSON(toJSON(json1))# the jsonlite package is used to reformat it as a dataframe 
# so I get the json structure that I got out from the original command to content
# and I convert that structured R object back into json by toJSON
# and then I use the jsonlite version of the fromJSON argument to create a data frame 
# this data frame is a data frame where each row corresponds to a tweet in my home timeline 
json[1, 1:4]# the first row and the first four columns

# Reading from other sources
# Interacting more directly with files
# file function - open a connection to a text file
# url - open a connection to a url

# for zipped files:
# gzfile - open a connection to a .gz file
# bzfile - open a connection to a bz2 file

# ? connections 

# foreign package
# loads data from Minitab, S, SAS, SPSS, Stata, Systat
# basic functions:
# read.arff (Weka)
# read.dta (Stata)
# read.mtp (Minitab)
# read.octave (Octave)
# read.spss (SPSS)
# read.xport (SAS)

# foreign package help file 
# http://cran.r-project.org/web/packages/foreign/foreign.pdf

# RPostgreSQL provides a DBI-compliant database connection from R
# https://code.google.com/p/rpostgresql/
# http://cran.r-project.org/web/packages/RPostgreSQL/RPostgreSQL.pdf

# ROBDC provides interfaces to multiple databases including PostgresQL, MySQL, Microsoft Access and SQLite
# http://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf
# http://cran.r-project.org/web/packages/RODBC/RODBC.pdf

# RMongo
# http://www.r-bloggers.com/r-and-mongodb/
# rmongodb # is another package that can be use to interface with mongodb and extract data from it

# when sending queries to database, you'll have to use database's own syntax
# so if you are going to use the R package, you have to learn a bit about the syntax for the database to be able to extract data

# Reading images
# jpeg - http://cran.r-project.org/web/packages/jpeg/index.html
# readbitmap - http://cran.r-project.org/web/packages/readbitmap/index.html
# png - http://cran.r-project.org/web/packages/png/index.html 
# EBImage(Bioconductor) - http://www.bioconductor.org/packages/2.13/bioc/html/EBImage.html

# Week2 Quiz Questions
# 1
# Register an application with the Github API here https://github.com/settings/applications. 
# Access the API to get information on your instructors repositories 
# (hint: this is the url you want "https://api.github.com/users/jtleek/repos"). 
# Use this data to find the time that the datasharing repo was created. What time was it created?
# This tutorial may be useful (https://github.com/hadley/httr/blob/master/demo/oauth2-github.r). You may also need to run the code in the base R package and not R studio.
# I also used this link: https://towardsdatascience.com/accessing-data-from-github-api-using-r-3633fb62cb08
# and this link: https://stackoverflow.com/questions/36732777/r-to-connect-to-github-api-but-an-credentials-error
install.packages("jsonlite")
library(jsonlite)
install.packages("httpuv")
library(httpuv)
library(httr)

# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")

# 2. To make your own application, register at
#    https://github.com/settings/developers. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app(appname = "MaryamOAuthApp",
                   key = "89608fcb427e6187c67d",
                   secret = "5a3387c61af642e29b43d477225c8a9450db708f")

# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)

# 4. Use API
gtoken <- config(token = github_token)
req <- with_config(gtoken, GET("https://api.github.com/users/jtleek/repos"))
con_req <- content(req)

find_create <- function(x,myurl) {
    if (x$html_url == myurl) {
        print(x$created_at)
    }
}

lapply(con_req,find_create,myurl ="https://github.com/jtleek/datasharing")

# 2
# The sqldf package allows for execution of SQL commands on R data frames. 
# We will use the sqldf package to practice the queries we might send with the dbSendQuery command in RMySQL.
install.packages("sqldf")

#library(RMySQL)
library(sqldf)
?sqldf

detach("package:RMySQL", unload=TRUE)
# Which of the following commands will select only the data for the probability weights pwgtp1 with ages less than 50?
sqldf("select * from acs2")
sqldf("select pwgtp1 from acs2 where AGEP < 50")# this is the right answer
sqldf("select pwgtp1 from acs2")
sqldf("select * from acs2 where AGEP < 50 and pwgtp1")

# Using the same data frame you created in the previous problem, what is the equivalent function to unique(acs$AGEP)
unique(acs2$AGEP)
sqldf("select distinct pwgtp1 from acs2")
sqldf("select AGEP where unique from acs2")
sqldf("select distinct AGEP from acs2")#this is the answer
sqldf("select unique AGEP from acs2")

# How many characters are in the 10th, 20th, 30th and 100th lines of HTML from this page:
# http://biostat.jhsph.edu/~jleek/contact.html
# (Hint: the nchar() function in R may be helpful)

con <- url("http://biostat.jhsph.edu/~jleek/contact.html")
?readLines
htmlCode <- readLines(con, n = 100L)
close(con)
htmlCode
nchar(htmlCode[c(10, 20, 30, 100)])

# 5
#Read this data set into R and report the sum of the numbers in the fourth of the nine columns.
#https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for
#Original source of the data: http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for
#(Hint this is a fixed width file format)

# The file extension .for is a critical clue about the file format
# It's a Fortran file that can be read with R functions that read fixed length records.  
# There are at least 3 different R functions that can be used to read this data, one of which is conveniently 
# named once you know what a .for file is.
originalSource <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(originalSource,"./data/wksst8110.for")
fileURL <- "./data/wksst8110.for"
View(fileURL)
# set addresses for fixed length fortran-style input file 
fwfCols <- c(10,5,4,4,5,4,4,5,4,4,5,4,4)
theColumns <- c("week","", "nino1and2sst","nino1and2ssta", "", "nino3sst",
                "nino3ssta", "", "nino34sst","nino34ssta", "", 
                "nino4sst","nino4ssta")
mydata2 <- read.fwf(fileURL,widths=fwfCols,skip=4,
                    col.names=theColumns)

colnames(mydata2) <- theColumns
View(mydata2)
class(mydata2)
sum(mydata2[,6], na.rm = T)


## Subsetting and sorting
# Once you have loaded your data into R, then you want to maipulate that data so to set ti up to be a tidy data set
# with variables in the columns and observations in the rows, and only the observations that you want to analyze

# To create a data frame with three variables
set.seed(13435)
X <- data.frame("var1" = sample(1:5), "var2" = sample(6:10), "var3" = sample(11:15))
X
# scramble up those variables so that they are not in an specific order and make some of the values missing
X <- X[sample(1:5), ]; X$var2[c(1,3)] = NA
X

# to subset a specific column
X[,1]
X[,"var1"]

# to subset by both rows and columns 
X[1:2, "var2"]

# Logicals ands and ors
X[(X$var1 <= 3 & X$var3 > 11), ]
X[(X$var1 <= 3 | X$var3 > 15), ]

# Dealing with missing values
X[which(X$var2 > 8), ]# with which you can subset the data set, even with the NAs that you might have

# Sorting
sort(X$var1)# sort the values in an increasing order
sort(X$var1, decreasing = T)
sort(X$var2, na.last = T)# when dealing with a variable that has missing values, you can tell it to put the NA values 
# at the end of the sort 

# Ordering 
X[order(X$var1), ]# order the data frame by a particular variable (attention you can not use sort function here to
# get the same results)

# to order by multiple variables
X[order(X$var1, X$var3), ]# it will first sort so that variable one is in increasing order, and then if they are multiple
# values of the variable 1 that are the same, it will sort the values of var3 in increasing order within those values

# Ordering with plyr package by Hadley Wickham
install.packages("plyr")
library(plyr)
arrange(X, var1)#var1 will be the variable that the data frame will be sorted on here 
arrange(X,desc(var1))#descending order 

# Adding rows and columns to data frames
X$var4 <- rnorm(5)#random normal vector of length 5
X

Y <- cbind(X, rnorm(5))# column bind the new vector on the right hand side of X 
Y
Y <- cbind(rnorm(5), X)# to add the column to the left side of X
Y
# rbind command can be used to bind the rows

# Further resources
# http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf

# Summarizing Data
if(!file.exists("./data")){dir.create("./data")}
# https://data.baltimorecity.gov/datasets/restaurants/data?geometry=-77.197%2C39.193%2C-76.044%2C39.379
# In the lecture, the data was downloaded and saved as CSV with in R but I couldn't do that so I downloaded from web on Chrome
# and then opened it here. The lines of the codes that were used in the lecutre are copied here, but not used:
# fileURL <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
# download.file(fileURL, destfile = "./data/restaurants.csv", method = "curl")
restData <- read.csv("./data/restaurants.csv")
View(restData)

head(restData, 3)
tail(restData, 3)

summary(restData)

str(restData)
# it tells us the class of restData, which is a data frame, the dimensions
# all the different classes that the different columns correspond to

quantile(restData$councilDistrict, na.rm = T)# the councilDistrict is not in the csv I have loaded, 
# I think it is replaced with cnsldst
quantile(restData$cncldst, na.rm = T)
quantile(restData$cncldst, probs = c(0.5, 0.75, 0.9))# to look at different probabilities 
# if I pass quantile probs, it will look at different quantiles of the distribution 

table(restData$zipcode, useNA = "ifany")# you can also make tables of the specific variables 
# you can also see where the bulk of zip code values are 
# the added argument here, useNA = "ifany", if there is any missing values, there will be an added column to this table
# which will be NA and it will tell you the number of missing values 
# by default the table function in R, does not tell you the number of missing values
# so it's important to use the command, useNA ifany to make sure that you are not missing the missing values 

# you can also make a 2 dimensional table
table(restData$cncldst, restData$zipcode)

# check for missing values
sum(is.na(restData$cnsldst))
any(is.na(restData$cnsldst))# any command will look through the entire set of values 
# and check for the trues 
all(restData$zipcode > 0)# all will check that all the values will satisfy the condition 
colSums(is.na(restData))# colSums and rowSums will take the sums across columns or rows 
all(colSums(is.na(restData)) == 0)# there are missing values in this dataset

table(restData$zipcode %in% c("21212"))# to find all the zipcodes that are equal to 21212
table(restData$zipcode %in% c("21212", "21213"))

# you can use the logical above to subset the data 
restData[restData$zipcode %in% c("21212", "21213"), ]# I can get the restaurants that are in 21212 and 21213
# so we get the rows in this dataset that had their zip codes eqal to either of these 
View(restData[restData$zipcode %in% c("21212", "21213"), ])
restData$name

data("UCBAdmissions")# loading data on Berkeley admissions
DF <- as.data.frame(UCBAdmissions)
summary(DF)# there are four variables, Admit, Gender, Dept, Freq

# Cross tabs
xt <- xtabs(Freq ~ Gender + Admit, data = DF)# this is to identify where the relationship exists in the data set
xt
# on the left, so Freq here, is the variable that you want to be displayed in the table 
# and you might want to break that down by some other variable, here Gender and whether they've been admitted or not

# Flat tables
warpbreaks$replicate <- rep(1:9, len = 54)
warpbreaks
xt <- xtabs(breaks ~., data = warpbreaks)# the value that appears in the table is equal to breaks 
# and that would be broken down by all the other variables in the dataset
# so breaks will be broken down by replicate, tension and wool
xt# mutliple two dimensional tables
ftable(xt)# making flat table to summarize data in a smaller and compact form so it is easier to see 

fakeData <- rnorm(1e5)
fakeData
object.size(fakeData)
print(object.size(fakeData), units = "Mb")

## Creating New Variables
# Creating sequences
# sequences are often used to index different operations thatt you are going to do on the data 
# the command to create sequences is seq, you give it the min and max values, and how many values to generate 
# using by and length
S1 <- seq(1,10, by = 2); S1# there are two ways to 

S2 <- seq(1, 10, length = 3); S2# it will start at 1 and finish ten and create 
# the same number of values as specified by length

x <- c(1, 3, 8, 25, 100); seq(along = x)# if you have a vector x with 5 values, and you want to create an index
# so that you can loop over those 5 values you can use seq along this vector x which will create a vector with the same
# length as x but with consecutive indices that you can use for looping or accessing subsets of data sets 

restData$nearMe <- restData$nghbrhd %in% c("Roland Park", "Homeland")
restData$nearMe
table(restData$nearMe)

# To create binary variables
restData$zipWrong <- ifelse(restData$zipcode < 0, TRUE, FALSE)# for the ifelse command, I first send it a condition 
# here, is the zipcode less than zero, the first thing that I return is TRUE if the condition holds TRUE  
# in other words all the cases where the zipcode is less than zero, I'll get a TRUE 
# and the function will return FALSE if the condition is FALSE 
sum(restData$zipWrong)
table(restData$zipWrong, restData$zipcode < 0)

# Creating categorical variables
# out of quantitative variables
# so we might want to break the zip codes into consecutive numbers
# so zipGroups, is a factor variable where we'll break the variable zipcode 
# up into the zero quantiles to the 25th percentile, the 25th to the 50th percentile...
# so for example, there are 375 values that land between the 25th percentile and the 50th percentile
restData$zipGroups <- cut(as.numeric(restData$zipcode), breaks = quantile(as.numeric(restData$zipcode), na.rm = T))
table(restData$zipGroups)
table(restData$zipGroups, restData$zipcode)
# So, what this does is it breaks the quantity of variable up into a categorical variable

# Easier cutting
install.packages("Hmisc")
install.packages("ggplot2")
install.packages("withr")

library(Hmisc)
library(ggplot2)
library(withr)

restData <- read.csv("./data/restaurants.csv", stringsAsFactors = F)
class(restData$zipcode)
restData$zipGroups <- cut2(as.numeric(restData$zipcode), g = 4)# to break the zipcodes into 4 different groups 
# and I want to break them up according to quantiles 
table(restData$zipGroups)

# Creating factor variables 
restData$zcf <- factor(restData$zipcode)
restData$zcf[1:10]#tell you about the different zip code levels
class(restData$zcf)

# Levels of factor variables
yesno <- sample(c("yes", "no"), size = 10, replace = T)
yesnofac <- factor(yesno, levels = c("yes", "no"))# by default, it will treat the lowest value alphabetically 
# as the first level of the factor variable. Here, it I want to treat the yes value as the lowest level, then I have to 
# specify it with the levels 
relevel(yesnofac, ref = "yes") 

as.numeric(yesnofac)# to change the factor variable back into a numeric variable. It will give the lowest level, 1

# Using the mutate function 
library(Hmisc)
library(plyr)

restData2 <- mutate(restData, zipGroups = cut2(as.numeric(zipcode), g = 4))# you can use the mutate function
# to create a new variable and simultaneously add it to the dataset. Here, we are going to apply the mutate function to the restData
# dataframe, and add the new variable zipGroups 
table(restData2$zipGroups)

# Common transforms
abs()#absolute value
sqrt()#square root

# to round values up or down 
ceiling()#ceiling(3.475) is 4
floor()#floor(3.475) is 3

# use round to round to as many digits you want:
round(x, digits = n)#round(3.475, digits = 2) is 3.48
# to rond to as many significant digits as you like 
signif(x, digits = n)# signif(3.475, digits = 2) is 3.5

cos()
sin()
log()#natural logarithm
log2()
log10()
exp()

# http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf
# http://statmethods.net/management/functions.html
# http://plyr.had.co.nz/09-user/
# http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf


## Reshaping data
# Often the data that you load into R, is not tidy
# the first that you want to do is to reshape the data into tidy data:
# 1. Each variable forms a column
# 2. Each observation forms a row. An observation is a case of the data being collected. For example, if we were collecting data
# on students in the class, the observations would be each individual student in the class. A variable is an attribute or characteristic
# of the observation we record in our data. 
# 3. Each table/file stores data about one kind of observation
# http://vita.had.co.nz/papers/tidy-data.pdf
# leek, Taub, and Pineda 2011 PloS One

install.packages("reshape2")
library(reshape2)
head(mtcars)

# Melting Data frames
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id = c("carname", "gear", "cyl"), measure.vars = c("mpg", "hp"))# we will have to tell the melt
# function, which are the variables are id variables and which of the variables are measure variables 
# in the variable column, we will have first mpg and then hp
head(carMelt)# the mpg is added to the top of the melted data set 
tail(carMelt)# the hp is added to the bottom 

# Casting data frames
# Once we have melted the data set we can recast it in different ways
# which is a way to re-summarize/re-organize the data set in different ways 
cylData <- dcast(carMelt, cyl ~ variable)# to have cylinder broken down by different variables 
# the dcast will recast the dataset into a dataframe 
?dcast
cylData
mtcars$cyl
# so here it will break down the values of cyl by the variables mpg and hp 
# so in a way it summarizes the data set and by default it does it by link 
# so what the cylData is saying is that for four cylinders we have 11 measures of mpg and 11 measures of horsepower,...

cylData <- dcast(carMelt, cyl ~ variable, mean)# here we are telling it to take the mean for each value of the variable
cylData# so for the four cylinder car, the mean miles per gallon is 26.66 and the mean horsepower is 82.62
# while for an 8 cylinder car, there is a much lower mean miles per gallon and a much higher horsepower 

# Averaging values  
head(InsectSprays)
tapply(InsectSprays$count, InsectSprays$spray, sum)# to take the sum of the count for each of the different sprays 
# I am going to apply the function sum to count along the index spray 
# within each value of spray, it will sum up the counts
# so you get the sum for A, B, C...

# http://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/

# Another way - split, apply, combine
spIns <- split(InsectSprays$count, InsectSprays$spray)
# to split the counts by each of the different sprays 
# now you will end up with a list, where you get the list of count values for each differet spray
spIns
# then you can apply a function to that list 
sprCount <- lapply(spIns, sum)# we will apply sum by lapply to each element of the list 
sprCount# now we have a list
class(sprCount)#list
# to go back to a vector, which is easier to manipulate in R, this is the combine part
unlist(sprCount)

# By using sapply, we can do both the apply and the combine functions
sprCount2 <- sapply(spIns, sum)
class(sprCount2)#numeric

# Another way - plyr package
library(plyr)
ddply(InsectSprays, .(spray), summarize, sum = sum(count))# the variable that we like to summarize and 
# you have to use .(variable)
# then we will say we want to summarize this variable by suming up the count within that variable 
# so the same processes of split, apply, combine are all done by this line of code 
?ddply
 
# to calculate the values and apply them to each variable
# for example, you want to subtract off the mean or the total count from the actual count for every variable 
# you can calculate a spraySums variable that is the same length as the original data set 
spraySums <- ddply(InsectSprays, .(spray), summarize, sum = ave(count, FUN=sum))# ave function which alone is basically the mean function
# that returns a vector of the same length as the input vector. The cool thing about ave function is that you can also apply any function
# FUN 
spraySums

# More information 
# A tutorial from the developer of plyr: http://plyr.had.co.nz/09-user/
# A nice reshape tutorial: http://www.slideshare.net/jeffreybreen/reshaping-data-in-r
# A good ply primer: http://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/

# see also the function
acast()# takes a melted data set and turns it into an array so it is for casting as multi-dimensional arrays
arrange()# for faster reordering without using order() commands
mutate()# adding new variables 
# so you can use a combination of ddply and mutate in order to add new variables that are summaries of the previous variables 

# Managing Data Frames with dplyr 
# dplyr package is specifically designed to help you work with data frames 
# The data frame is a key data structure in statistics and R 
# The basic assumptions that are made by dplyr package are that in a give data frame, each observation 
# is represented by one row and each variable is represented by a column (tidy data) 

# There are a number of different implementations of the dplyr package, but we'll use for this 
# presentation just the default R implementation. But you can use dplyr with data table package and 
# relational database systems
# the dplyr package was developed by Hadley Wickham
# it is an optimized and distilled version of plyr package (also by Hadley)
# does not provide any "new" functionality per se, but greatly simplifies existing functionality in R 
# provides a "grammar" (in particular, verbs) for data maipulation 
# Is very fast, as many key operations are coded in C++
library(dplyr)
options(width = 105)
select()# return a subset of the columns of a data frame 
filter()# extract a subset of rows from a data frame based on logical conditions 
arrange()# reorder rows of a data frame 
rename()# rename variables in a data frame 
mutate()# add new variables/columns or transform existing variables

summarise()# or
summarize()# generate summary statistics of different variables in the data frame, possibly witin strata
# There is also a handy print method that prevents you from printing a lot of data to the console. 

# The first argument to all dplyr functions is a data frame 
# The subsequent arguments describe what to do with it, and you can refer to columns in the data frame directly without using
# the $ operator (just use the names)
# The result is a new data frame
# Data frames must be properly formatted and annotated for this to all be useful 

# Managing Data Frames with dplyr - Basic Tools 
library(dplyr)
chicago <- readRDS("chicago.rds")
dim(chicago)
str(chicago)
names(chicago)# to look at the variable names 

# select
head(select(chicago, city:dptp))# so we can look at subsets of columns by just refering to their names 
head(select(chicago, -(city:dptp)))# to look at all the columns except for the columns indicated by the - range 

head(chicago[, -(1: 3)])

i <- match("city", names(chicago))# match returns a vector of the positions of (first) matches of its first argument in its second
j <- match("dptp", names(chicago))
head(chicago[, -(i:j)])

# filter
chic.f <- filter(chicago, pm25tmean2 > 30)
chic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)
chic.f

# arrange: used to reorder the rows of the a dataframe based on the values of a column
chicago <- arrange(chicago, date)# to order the rows of the dataframe according to the date variables
head(chicago)# 1987
tail(chicago)# 2005

chicago <- arrange(chicago, desc(date))
head(chicago)# 2005
tail(chicago)# 1987

# rename: used to rename a variable in R
chicago <- rename(chicago, pm25 = pm25tmean2, dewpoint = dptp)# new name = old name 
names(chicago)

# mutate is to transform existing variables or to create new variables 
chicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = T))
head(chicago)
head(select(chicago, pm25, pm25detrend))

# group_by function allows you split the data frame according to certain categorical variables 
chicago <- mutate(chicago, tempcat = factor(1 * (tmpd > 80), labels = c("cold", "hot")))# to create a tempreture categorical variable 
# which would indicate whether a given day was hot or cold depending on whether the tempreture was over 80 degrees or not 
# a factor variable 
?factor
chicago
class(chicago$tmpd > 80)# logical 
class(1 * (chicago$tmpd > 80))# numeric 
chicago <- mutate(chicago, tempcat = factor(tmpd > 80, labels = c("cold", "hot")))# this code works as well
head(chicago)
hotcold <- group_by(chicago, tempcat)# to use group_by to create a new data structure based on the original data frame and the 
# and the new variable tempcat

hotcold# notice the slightly different format 

# summerize
# to summarize the hotcold object which has been split based on the tempreture category variable  
# to know what is the mean pm25 for both hot and cold days, what is the maximum ozone for hot and cold days
# and what is the median no2 for hot and cold days 
summarize(hotcold, pm25 = mean(pm25, na.rm = T), o3 = max(o3tmean2), no2 = median(no2tmean2))

# to do a summary on each year in the data set 
chicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)
as.POSIXlt("1987-01-01")$year#87
# then I can group_by the chicago data set by year 
years <- group_by(chicago, year)
summarize(years, pm25 = mean(pm25, na.rm = T), o3 = max(o3tmean2), no2 = median (no2tmean2))

# pipeline operator %>%
# allows you to chain different operations together
# and does it in a way that allows you to see what operations are happening in a readable way 
# the idea is that you take a data st and you feed it through a pipeline of operations to create a new data set 

# to mutate the chicago data set to create a month variable because I want to create a summary of each of 
# the pollutant variables by month 
# so I create the month variable by mutate and then I take the output of mutate and group_by it 
# according to the month variable and then I get the output of group by and run it through summarize 
chicago %>% mutate (month = as.POSIXlt(date)$mon + 1) %>% group_by(month) %>% summarize(pm25 = mean(pm25, na.rm = T), o3 = max(o3tmean2),
                                                                                        no2 = median(no2tmean2))
# notice that when using the pipeline operator, I don't have to specify the data frame as the first argument 
# that is implied by the use of pipeline operator 
# so the output of this pipeline operation is a data frame that shows you the summary stat of each of the three
# pollutant variables by each of the 12 month of the year
# the pipeline operator is a really handy tool because it prevents you from having to assign a number of temporary variables that 
# you subsequently fed into another function 

# you can use dplyr package with other data frame "backends"
# we just used the default R implementation here 
# data.table package for large fast tables 
# SQL interface for relational databases via the DBI package 

# Merging data 
# Usually we want to match data sets based on ID, very similar to the idea of having a linked set of tables and databases 
# like MySQL 

if(!file.exists("./data")){dir.create("./data")}
fileURL1 <- "https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/03_GettingData/04_01_editingTextVariables/data/reviews.csv"
fileURL2 <- "https://raw.githubusercontent.com/DataScienceSpecialization/courses/master/03_GettingData/04_01_editingTextVariables/data/solutions.csv"
download.file(fileURL1, destfile = "./data/reviews.csv", method = "curl")
download.file(fileURL2, destfile = "./data/solutions.csv", method = "curl")
reviews <- read.csv("./data/reviews.csv"); solutions <- read.csv("./data/solutions.csv")
head(reviews, 3)# solution id in reviews data frame corresponds to id in the solutions data frame
head(solutions, 3)

names(reviews)
names(solutions)

mergedData <- merge(reviews, solutions, by.x = "solution_id", by.y = "id", all = T)# first you have to give merge the two datasets to merge 
# then by or by.x and by.y to tell merge which of the columns is should merge by 
# by default it would merges by all of the columns that have the common name 
# so for example in this data set it would try to merge by id, start, stop, time_left
# all = T means that if there's a value that appears in one but not in the other, it should include 
# another row with na values for the missing values that don't appear in other data frame 
View(reviews)
View(solutions)
View(mergedData) 

head(reviews, 3)
head(solutions, 3)
head(mergedData, 3) 

# merge Default - merge all common column names
intersect(names(solutions), names(reviews))
mergedData2 <- merge(reviews, solutions, all = T)
head(mergedData2)# the id variable matches up but the start and stop variables will not necessarily match up 
# so it would create a data frame that is larger and have multiple rows one for each row of reviews and one for each row of solutions 

# join in the plyr package 
# it can only merge on the basis of common rows or common names between the two datasets 
# so it wouldn't have been able to merger the reviews and solutions data frames based on solution_id and id 
# but if you have multiple data sets it is relatively challenging to do it by merge 
# but if they have a common id it is very straightforward to do it with the join command 

# here we make to data frames with a common id identifier 
df1 <- data.frame(id = sample(1:10), x = rnorm(10))
df1
df2 <- data.frame(id = sample(1:10), y = rnorm(10))
df2
df3 <- data.frame(id = sample(1:10), z = rnorm(10))
df3
joined1and2 <- join(df1, df2)
joined1and2# it will join them together by id
arrange(joined1and2, id)

dfList <- list(df1, df2, df3)# putting the data frames in one big list so this is a data frame list with 
# 3 data frames in it 
dfList
join_all(dfList)# merges all the different data frames on the basis of common variables, here id 

# More on merging data 
# The quick R data merging page - http://www.statmethods.net/management/merging.html
# plyr information - http://plyr.had.co.nz/
# Types of joins - http://en.wikipedia.org/wiki/join_(SQL)) # read about the different kinds of joing, left joins and right joins 

# swirl - Getting and Cleaning Data - Manipulating Data with dplyr
# dplyr _ rovides a consistent and concise grammar for manipulating tabular data
#  One unique aspect of dplyr is that the same set of tools allow you to work with tabular data from a variety of sources, 
# including data frames, data tables, databases and multidimensional arrays.

# As you may know, "CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for
# R" (http://cran.rstudio.com/). RStudio maintains one of these so-called 'CRAN mirrors' and they generously make their download logs publicly
# available (http://cran-logs.rstudio.com/). We'll be working with the log from July 8, 2014, which contains information on roughly 225,000 package
# downloads.

# I've created a variable called path2csv, which contains the full file path to the dataset. Call read.csv() with two arguments, path2csv and
# stringsAsFactors = FALSE, and save the result in a new variable called mydf. 

packageVersion("dplyr")

# The first step of working with data in dplyr is to load the data into what the package authors call a 'data frame tbl' or 'tbl_df'. Use the following
# code to create a new tbl_df called cran:
cran <- tbl_df(mydf)

# To avoid confusion and keep things running smoothly, let's remove the original data frame from your workspace with rm("mydf").
rm("mydf")
# From ?tbl_df, "The main advantage to using a tbl_df over a regular data frame is the printing." Let's see what is meant by this. Type cran to print
# our tbl_df to the console.
cran
# First, we are shown the class and dimensions of the dataset. Just below that, we get a preview of the data. Instead of attempting to print the entire
# dataset, dplyr just shows us the first 10 rows of data and only as many columns as fit neatly in our console. At the bottom, we see the names and
# classes for any variables that didn't fit on our screen.

# According to the "Introduction to dplyr" vignette written by the package authors, "The dplyr philosophy is to have small functions that each do one
# thing well." Specifically, dplyr supplies five 'verbs' that cover most fundamental data manipulation tasks: select(), filter(), arrange(), mutate(),
# and summarize().

# Normally, the : notation is reserved for numbers, but select() allows you to specify a sequence of columns this way, which can save a bunch of typing.
# Use select(cran, r_arch:country) to select all columns starting from r_arch and ending with country.
# select(cran, country:r_arch)

# Instead of specifying the columns we want to keep, we can also specify the columns we want to throw away. To see how this works, do select(cran,
# -time) to omit the time column.

# The negative sign in front of time tells select() that we DON'T want the time column. Now, let's combine strategies to omit all columns from X
# through size (X:size). 

# How that you know how to select a subset of columns using select(), a natural next question is "How do I select a subset of rows?" That's where the
# filter() function comes in.
filter(cran, package == "swirl")
# Again, note that filter() recognizes 'package' as a column of cran, without you having to explicitly specify cran$package.

# The == operator asks whether the thing on the left is equal to the thing on the right. If yes, then it returns TRUE. If no, then FALSE. In this case,
# package is an entire vector (column) of values, so package == "swirl" returns a vector of TRUEs and FALSEs. filter() then returns only the rows of
# cran corresponding to the TRUEs.


# You can specify as many conditions as you want, separated by commas. For example filter(cran, r_version == "3.1.1", country == "US") will return all
# rows of cran corresponding to downloads from users in the US running R version 3.1.1.
filter(cran, r_version == "3.1.1", country == "US")
filter(cran, country == "IN", r_version <= "3.0.2")
# Our last two calls to filter() requested all rows for which some condition AND another condition were TRUE. We can also request rows for which EITHER
# one condition OR another condition are TRUE. For example, filter(cran, country == "US" | country == "IN") will gives us all rows for which the
# country variable equals either "US" or "IN". Give it a go.
filter(cran, country == "US" | country == "IN")
# Now, use filter() to fetch all rows for which size is strictly greater than (>) 100500 (no quotes, since size is numeric) AND r_os equals
# "linux-gnu". Hint: You are passing three arguments to filter(): the name of the dataset, the first condition, and the second condition.
filter(cran, size > 100500, r_os == "linux-gnu")


# Finally, we want to get only the rows for which the r_version is not missing. R represents missing values with NA and these missing values can be
# detected using the is.na() function.
is.na(c(3, 5, NA, 10))
# Now, put an exclamation point (!) before is.na() to change all of the TRUEs to FALSEs and all of the FALSEs to TRUEs, thus telling us what is NOT NA:
# !is.na(c(3, 5, NA, 10)).

# Okay, ready to put all of this together? Use filter() to return all rows of cran for which r_version is NOT NA. Hint: You will need to use !is.na()
# as part of your second argument to filter().

# We've seen how to select a subset of columns and rows from our dataset using select() and filter(), respectively. Inherent in select() was also the
# ability to arrange our selected columns in any order we please.
# Sometimes we want to order the rows of a dataset according to the values of a particular variable. This is the job of arrange().
cran2 <- select(cran, size:ip_id)
# Now, to order the ROWS of cran2 so that ip_id is in ascending order (from small to large), type arrange(cran2, ip_id). You may want to make your
# console wide enough so that you can see ip_id, which is the last column.
arrange(cran2, desc(ip_id))
# We can also arrange the data according to the values of multiple variables. For example, arrange(cran2, package, ip_id) will first arrange by package
# names (ascending alphabetically), then by ip_id. This means that if there are multiple rows with the same value for package, they will be sorted by
# ip_id (ascending numerically).
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)

# It's common to create a new variable based on the value of one or more variables already in a dataset. The mutate() function does exactly this.
mutate(cran3, size_mb = size / 2^20)
# One very nice feature of mutate() is that you can use the value computed for your second column (size_mb) to create a third column, all in the same
# line of code. To see this in action, repeat the exact same command as above, except add a third argument creating a column that is named size_gb and
# equal to size_mb / 2^10.

# The last of the five core dplyr verbs, summarize(), collapses the dataset to a single row. Let's say we're interested in knowing the average download
# size. summarize(cran, avg_bytes = mean(size)) will yield the mean value of the size variable. Here we've chosen to label the result 'avg_bytes', but
# we could have named it anything. 
summarize(cran, avg_bytes = mean(size))
# summarize() is most useful when working with data that has been grouped by the values of a particular variable.
# The main idea behind grouping data is that you want to break up your dataset into groups of rows based on the values of one or more variables. The
# group_by() function is reponsible for doing this.

# Grouping and Chaining with dplyr
# I've made the dataset available to you in a data frame called mydf. Put it in a 'data frame tbl' using the tbl_df() function and store the result in
# a object called cran. 

# Group cran by the package variable and store the result in a new object called by_package.
by_package <- group_by(cran, package)
# Groups:   package [6,023]
# At the top of the output above, you'll see 'Groups: package', which tells us that this tbl has been grouped by the package variable. Everything else
# looks the same, but now any operation we apply to the grouped data will take place on a per package basis.

# Recall that when we applied mean(size) to the original tbl_df via summarize(), it returned a single number -- the mean of all values in the size
# column. We may care about what that number is, but wouldn't it be so much more interesting to look at the mean download size for each unique package?
summarize(by_package, mean(size))
# Instead of returning a single value, summarize() now returns the mean size for EACH package in our dataset.
# Compute four values, in the following order, from
# the grouped data:
#
# 1. count = n()
# 2. unique = n_distinct(ip_id)
# 3. countries = n_distinct(country)
# 4. avg_bytes = mean(size)
#
# A few thing to be careful of:
#
# 1. Separate arguments by commas
# 2. Make sure you have a closing parenthesis
# 3. Check your spelling!
# 4. Store the result in pack_sum (for 'package summary')
#
# You should also take a look at ?n and ?n_distinct, so
# that you really understand what is going on.

pack_sum <- summarize(by_package,
                      count = n(),
                      unique = n_distinct(ip_id),
                      countries = n_distinct(country),
                      avg_bytes = mean(size))

# The 'count' column, created with n(), contains the total number of rows (i.e. downloads) for each package. The
# 'unique' column, created with n_distinct(ip_id), gives the total number of unique downloads for each package, as
# measured by the number of distinct ip_id's. The 'countries' column, created with n_distinct(country), provides the
# number of countries in which each package was downloaded. And finally, the 'avg_bytes' column, created with
# mean(size), contains the mean download size (in bytes) for each package.

# Naturally, we'd like to know which packages were most popular on the day these data were collected (July 8, 2014).
# Let's start by isolating the top 1% of packages, based on the total number of downloads as measured by the 'count'
# column.

# We need to know the value of 'count' that splits the data into the top 1% and bottom 99% of packages based on total
# downloads. In statistics, this is called the 0.99, or 99%, sample quantile. Use quantile(pack_sum$count, probs =
# 0.99) to determine this number.
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)

# Perhaps we're more interested in the number of *unique* downloads on this particular day. In other words, if a
# package is downloaded ten times in one day from the same computer, we may wish to count that as only one download.
# That's what the 'unique' column will tell us.

# Chaining allows you to string together multiple function calls in a way that is compact and readable, while still
# accomplishing the desired result. To make it more concrete, let's compute our last popularity metric from scratch,
# starting with our original data.

# Don't change any of the code below. Just type submit()
# when you think you understand it.

# We've already done this part, but we're repeating it
# here for clarity.

by_package <- group_by(cran, package)
pack_sum <- summarize(by_package,
                      count = n(),
                      unique = n_distinct(ip_id),
                      countries = n_distinct(country),
                      avg_bytes = mean(size))

# Here's the new bit, but using the same approach we've
# been using this whole time.

top_countries <- filter(pack_sum, countries > 60)
result1 <- arrange(top_countries, desc(countries), avg_bytes)

# Print the results to the console.
print(result1)

# It's worth noting that we sorted primarily by country, but used avg_bytes (in ascending order) as a tie breaker. This
# means that if two packages were downloaded from the same number of countries, the package with a smaller average
# download size received a higher ranking.

# We'd like to accomplish the same result as the last script, but avoid saving our intermediate results. This requires
# embedding function calls within one another.

# In this script, we've used a special chaining operator, %>%, which was originally introduced in the magrittr R
# package and has now become a key component of dplyr. You can pull up the related documentation with ?chain. The
# benefit of %>% is that it allows us to chain the function calls in a linear fashion. The code to the right of %>%
# operates on the result from the code to the left of %>%.

# Read the code below, but don't change anything. As
# you read it, you can pronounce the %>% operator as
# the word 'then'.
#
# Type submit() when you think you understand
# everything here.

result3 <-
    cran %>%
    group_by(package) %>%
    summarize(count = n(),
              unique = n_distinct(ip_id),
              countries = n_distinct(country),
              avg_bytes = mean(size)
    ) %>%
    filter(countries > 60) %>%
    arrange(desc(countries), avg_bytes)

# Print result to console
print(result3)

# To help drive the point home, let's work through a few more examples of chaining.
# select() the following columns from cran. Keep in mind
# that when you're using the chaining operator, you don't
# need to specify the name of the data tbl in your call to
# select().
#
# 1. ip_id
# 2. country
# 3. package
# 4. size
#
# The call to print() at the end of the chain is optional,
# but necessary if you want your results printed to the
# console. Note that since there are no additional arguments
# to print(), you can leave off the parentheses after
# the function name. This is a convenient feature of the %>%
# operator.

cran %>%
    select(ip_id, country, package, size) %>%
    print

# Use mutate() to add a column called size_mb that contains
# the size of each download in megabytes (i.e. size / 2^20).
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20) %>%
    print

# Use filter() to select all rows for which size_mb is
# less than or equal to (<=) 0.5.
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20) %>%
    filter(size_mb <= 0.5) %>%
    print

# arrange() the result by size_mb, in descending order.
#
# If you want your results printed to the console, add
# print to the end of your chain.

cran %>%
    select(ip_id, country, package, size) %>%
    mutate(size_mb = size / 2^20) %>%
    filter(size_mb <= 0.5) %>%
    arrange(desc(size_mb)) %>%
    print

# Tidying Data with tidyr: In this lesson, you'll learn how to tidy your data with the tidyr package.
library(tidyr)
# The author of tidyr, Hadley Wickham, discusses his philosophy of tidy data in his 'Tidy Data' paper:
# http://vita.had.co.nz/papers/tidy-data.pdf
# Tidy data is formatted in a standard way that facilitates exploration and analysis and works seamlessly with other tidy data tools. Specifically,
# tidy data satisfies three conditions:
# 1) Each variable forms a column
# 2) Each observation forms a row
# 3) Each type of observational unit forms a table

# Week3 Exam
# Q3
# Load the Gross Domestic Product data for the 190 ranked countries in this data set:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv 
# Load the educational data from this data set:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv
# Match the data based on the country shortcode. How many of the IDs match? Sort the data frame in descending order by GDP rank (so United States is last). What is the 13th country in the resulting data frame?
# Original data sources: 
# http://data.worldbank.org/data-catalog/GDP-ranking-table
# http://data.worldbank.org/data-catalog/ed-stats

fileurl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fileurl, destfile = "./data/gdp.csv", method = "curl")
gdpData <- read.csv("./data/gdp.csv")
head(gdpData)
View(gdpData)

fileurl2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(fileurl2, destfile = "./data/edu.csv", method = "curl")
eduData <- read.csv("./data/edu.csv")
View(eduData)

mergedgdpedu <- merge(gdpData, eduData, by.x = "X", by.y = "CountryCode", all = F)
View(mergedgdpedu)
names(mergedgdpedu)
arrangedgdpedu <- arrange(mergedgdpedu, Gross.domestic.product.2012)
View(arrangedgdpedu)
arrangedgdpedu2 <- arrangedgdpedu[-(1:35), ]
View(arrangedgdpedu2)
nrow(arrangedgdpedu2)#189
names(arrangedgdpedu2)
as.numeric(as.character(arrangedgdpedu2$Gross.domestic.product.2012))

arranged_gdp_edu <- arrange(arrangedgdpedu2, desc(as.numeric(as.character(arrangedgdpedu2$Gross.domestic.product.2012))))
View(arranged_gdp_edu)#13th country is St. Kitts and Nevis

# Q4 What is the average GDP ranking for the "High income: OECD" and "High income: nonOECD" group?
names(arranged_gdp_edu)
class(arranged_gdp_edu$Gross.domestic.product.2012)#factor
arranged_gdp_edu3 <- mutate(arranged_gdp_edu, Gross.domestic.product.2012 = (as.numeric(as.character(Gross.domestic.product.2012))))
View(arranged_gdp_edu3)

#arranged_gdp_edu2 <- mutate(arranged_gdp_edu, Gross.domestic.product.2012 = as.numeric(as.character(arrangedgdpedu2$Gross.domestic.product.2012)))
#View(arranged_gdp_edu2)
#class(arranged_gdp_edu2$Gross.domestic.product.2012)

by_Income <- group_by(arranged_gdp_edu3, Income.Group)
by_Income
View(by_Income)
View(summarize(by_Income, ave(Gross.domestic.product.2012)))#32.96, 91.91

# Question 5
# Cut the GDP ranking into 5 separate quantile groups. Make a table versus Income.Group. How many countries
# are Lower middle income but among the 38 nations with highest GDP?

quantile(arranged_gdp_edu3$Gross.domestic.product.2012, probs = c(0.2, 0.4, 0.6, 0.8, 1))
highestgdp <- arranged_gdp_edu3[152:189, ]
View(highestgdp)
sum(highestgdp$Income.Group == "Lower middle income", na.rm = T)# 5

# Week 4 - Editing text variables 
# A very common data cleaning step is to have text variables that are in sort of a nasty format or have extra spaces, periods or underscores that you need to remove.
# This lecture is about how to get nice, decent variable names and texts in the actual data set itself
cameraData <- read.csv("./data/cameras.csv")
names(cameraData)
tolower(names(cameraData))# to change the names to all lower case letters, there is also a toupper() command if you need that 

strsplit()# string split command is good for automatically splitting variable names 
# important parameters: x, split
splitNames <- strsplit(names(cameraData), "\\.")# to split on periods
# you have to use escape character \\ because the period is a reserved character 
names(cameraData)
splitNames
#[[1]]
#[1] "address"

#[[2]]
#[1] "direction"

#[[3]]
#[1] "street"

#[[4]]
#[1] "crossStreet"

#[[5]]
#[1] "intersection"

#[[6]]
#[1] "Location" "1"       

#[[7]]
#[1] "X2010"         "Census"        "Neighborhoods"

#[[8]]
#[1] "X2010"     "Census"    "Wards"     "Precincts"

#[[9]]
#[1] "Zip"   "Codes"

splitNames[[5]]
splitNames[[6]]# to subset out and take the part of the variable name that does not have the dot in it

mylist <- list(letters = c("A", "B", "C"), numbers = 1:3, matrix(1:25, ncol = 5))
head(mylist)
class(mylist[1])#list
class(mylist[[1]])#character
class(mylist$letters)#character

sapply(list, function)# applies a function to each element in a vector or list
# important parameters are x and function

firstElement <- function(x) {x[1]}
sapply(splitNames, firstElement)

reviews <- read.csv("./data/reviews.csv")# the reviews is the set of problems that have been reviewed by peers 
solutions <- read.csv("./data/solutions.csv")# the solutions is the set of the SAT questions that have been submitted by people 
head(reviews)
head(solutions)

names(reviews)# I want to be able to substitute out characters 
# In this case a couple of the variable names are solution_id, reviewer_id and time_left have underscores in them
# To remove the underscores
sub("_", "", names(reviews), )# to substitute _ with nothing in the names of reviews 

testName <-"this_is_a_test"
sub("_", "", testName)# only replaces the first underscore 
gsub("_", "", testName)# replaces all underscores

# searching for specific values 
grep("Alameda", cameraData$intersection)# to include all the intersections that include Alameda as one of the roads 
# the grep will take as input a search string that you want to look for, here the Alameda and it will look through the intersection variable 
# and find all of the instances that Alameda appears
head(cameraData$intersection)
cameraData$intersection[c(65, 69, 79)]
cameraData$intersection[c(4, 5, 36)]
grepl("Alameda", cameraData$intersection)# will return a vector of TRUE and FALSE, TRUE for whenever Alameda appears 
table(grepl("Alameda", cameraData$intersection))

cameraData2 <- cameraData[!grepl("Alameda", cameraData$intersection), ]# to subet to only the subset of data that Alameda does not appear 
cameraData2

grep("Alameda", cameraData$intersection, value = T)# this will return the values where Alameda appears 
grep("JeffStreet", cameraData$intersection)
length(grep("JeffStreet", cameraData$intersection))# 0

library(stringr)# provides useful string functions 
nchar("Jeffrey Leek")# the number of characters in a string 
substr("Jeffrey Leek", 1, 7)# to take out part of the string, to find first through the seventh letter  
paste("Jeffrey", "Leek")# to paste two strings together, then I get one string that is separated by space 
paste0("Jeffrey", "Leek")# to paste things with no space in between

str_trim("Jeff  ")# this function is from the stringr package and trims off any excess space 
# that appears at the beginning or end of a string 

# Importnat points about text in data sets
# Names of variables should be:
# 1. all lower cases when possible
# 2. descriptive (diagnosis vs Dx)
# 3. not duplicated
# 4. not have underscores or dots or white spaces

# Variables with character values
# 1. should usually be made into factor variables (depends on application)
# 2. should be descriptive (use TRUE/FALSE instead of 0/1 and Male/Female or M/F instead of 0/1)

# Regular Expressions 
# to search for a specific bit of text 
# Regular expressions can be thought of as a combination of literals and metacharacters
# To draw an analogy with natural language, think of literal text forming the words of this language
# and the metacharacters defining its grammers
# Regular expressions have a rich set of metacharacters that allow us to search through 
# strings to identify specific patterns of interest that might be very hard to identify with literals 

# Literals
# Literals consist of words that match exactly 
# Simplest patterns consists of only literals. The literal "nuclear" would match to the following lines:
# Laozi says nuclear weapons are mas macho 
# Chaos in a country that has nuclear weapons - not good.
# my nephew is trying to teach me nuclear physics
# lol if you ever try to say "nuclear" people immediately think DEATH by radiaiton LOL

# The literal "Obama" would match to the following lines:
# Clinton conceeds to Obama but will her followes listen??
# Are we sure Chelsea didn't vote for Obama?
# thinking...Michelle Obama is terrific!

#  Simplest pattern consists only of literals; a match occurs if the sequence of literals occurs
# anywhere in the text being tested
# What if we want only the word "Obama"? or sentences that end in the word "Clinton", or "clinton" or
# "clinto"
# We need a way to express: whitespace word boundaries, sets of literals, the begining and end of a line
# alternatives ("war" or "peace") metacharacters
# so, what we're going to use here are metacharacters in order to specify these 
# more general search terms that we'd like to be able to identify 

# Metacharacters 
# ^ represent the start of a line: ^i think will match the lines:
# i think we all rule for participating
# i think i have been outed
# i think this will be quite fun actually 
# i think i need to go to work 
# (so this would not match i think if it would came somewhere in the middle of the line)

# $ represents the end of a line: morning$ will match the end of a line 
# well they had something this morning
# then had to catch a tram home in the morning
# dog obedience school in the morning 
# good morning 

# We can list a set of characters we will aceept at any given point in the match 
# [Bb] [Uu] [Ss] [Hh] will match the lines (attention: I am not sure whether it is okay to have the spaces
# in between here)
# so either a captial or lower case of these letters, so it would match all versions of the 
# word bush regardless of which letters are capitalized 
# The democrats are playing, "Name the worst thing about Bush!"
# BBQ and bushwalking at Molonglo Gorge 
# Bush told you about North Korea
# I'm listening to Bush 

# ^[Ii] am will match 
# so this says that I want to look at the begining of a line to see either a capital I or a lowercase i
# and then the literal am 
# i am so angry at my boyfriend
# I am twittering from iPhone

# You can specify range of letters [a-z] or [a-zA-Z]: notice that the order doesn't matter 
# [a-z] match any letter from a to z that's lower case 
# [a-zA-Z] will look for ay of the letters whether they're lowercase or uppercase
# ^[0-9][a-zA-Z] will match the lines 
# will start at the begining of the line, will look for any number between 0 and 9, followed by a letter 
# 7th inning stretch
# 2nd half 
# 3am - cant sleep
# 5ft 7 sent from heaven 
# 1st sign of 

# When used at the begining of a character class, the ^ is also a metacharacter and indicates
# that it should match any of the characters not in the indicated class 
# [^?.]$ will match the lines 
# looking for any line that ends in anything other than a question mark or period 
# i like basketballs 
# 6 and 9 
# don't worry... we all die anyway!
# Not in Baghdad
# helicopter under water? hmm

# "." is used to refer to any characters so: 9.11 will match the lines because 9 and 11 are separated
# by exactly one character 
# its stupid the post 9-11 rules 
# if any 1 of us did 9/11 we should have been caught in days
# NetBios: scanning ip 203.169.114.66
# Front Door 9:11:46 AM

# | or metacharacter; we can use it to combine two expressions, the subexpressions being called alternatives
# flood|fire will match the lines:
# is firewire like usb on none macs?
# the global flood makes sense within the context of the bible
# yeah ive had the fire on 
# ... and the floods, hurricanes

# We can include any number of alternatives 
# flood|earthquake|hurricane|coldfire will match the lines 
# Not a whole lot of hurricanes in the Arctic
# We do have earthquakes nearly every day somewhere in our State
# hurricanes swirl in the other direction
# coldfire is STRAIGHT! 

# The alternatives can be expressions and not just literals
# ^[Gg]ood|[Bb]ad
# What we are searching here is the begining of the line and then either a capital or lower case good
# or anywhere in the line a capital or lower case Bad or bad
# good to hear some good news from someone here
# Good afternoon
# Katie..guess they had bad experiences

# Subexpressions are often contained in parentheses to constrain the alternatives
# ^ ([Gg]ood|[Bb]ad)
# looking at the begining of the line and you have to have either good or bad 
# at the begining of the line 
# bad habbit
# bad coordination 
# good, because there is..
# Badcop, its because people want to use drugs
# Good Monday

# The question mark indicates that the indicated expression is optional
# a question mark that follows a parentheses, it suggests that the indicated expression is optional  
# [Gg]eorge( [Ww]\.)? [Bb]ush will match the lines:
# (\ is to escape the character dot and that's because dot can be used as a metacharacter 
# we the backslash, we have said that don't consider this to be a metacharacter) 
# (we wanted to match a "." as a literal period; to do that,
# we had to "escape" the metacharacter, perceding it with a backslash. In general, we have to do this
# for any metacharacter we want to include in our match)
# i bet i can spell better than you and george bush combined
# BBC reported that President George W. Bush claimed God told him to invade I
# a bird in the hand is worth two george bushes 

# * and + are metacharacters
# * means repeat any number of times including none of the item  
# + means at least one of the item 
# (.*) will match the lines:
# so we are searching for sth between parenthesis and it could be any character repeated any number of times
# anyone wanna chat? (24, m, germany)
# hello, 20.m here....(east area + drives + webcam)
# (he means older men)
# ()

# [0-9]+ (.*)[0-9]+ will match the lines:
# looking for at least one number, followed by any number of characters, followed by at least one number again
# so this would allow us to look for any combination of numbers that are separated by something 
# other than numbers 
# working as MP here 720 MP battallion, 42nd birgade 
# so say 2 or 3 years at college and 4 at uni makes us 23
# it went down on several occasions for like, 3 or 4 *days*
# Mmm its time 4 me 2 go 2 bed 

# {and} are referred to as interval quantifiers; let us specify the minimum and maximum 
# number of matches of an expression 
# [Bb]ush( +[^ ]+ +){1,5} debate wiil match the lines: 
# In between [Bb]ush and debate, what we are looking for, is at least one space, followed by sth 
# that is not a space, followed by at least one space, and we want to see that between 1 and 5 times
# In other words, we want to see space word space, between one and five times 
# so as long as there's between one and five word between the two
# Bush has historically won all major debates he's done 
# In my view, Bush doesn't need these debates...
# That's what bush supporters are doing about the debate.

# {m, n} means at least m times but not more than n times  
# {m} means exactly m matches
# {m,} means at least m matches 

# In most implementations of regular expressions, the parentheses not only limit the scope of 
# alternatives divided by "|", but also can be used to "remember" text matched by the 
# subexpression enclosed 
# we refer to the matched text with escaped numbers \1, \2, etc
#  +([a-zA-Z]+) +\1 + will match the lines 
# a space followed by at least one but possibly more characters followed by at least one space
# followed by the exact same match that we saw within the parenthesis
# time for bed, night night twitter!
# blah blah blah blah
# my tattoo is so so itchy today 
# i was standing all all alone against the world outside 
# estudiando css css css ... que 

# The * is "greedy" so it always matches the longest possible string that satisfies the 
# regular expession 
# ^s(.*)s will match all these lines 
# so we are starting at the begining of a string and we are looking for an s followed by 
# some possibly large number of characters followed by another s
# so its gonna match the entire phrases below, because it goes for the longest possible 
# string that satisfies the expression 
# and that is true for all of the below phrases because they all start with an s and finish in an s
# sitting at starbucks
# setting up mysql and rails 
# studying stuff for the exams
# sore shoulders, stupid ergonomics 

# the greediness of * can be turned off with the ?, as in
# ^s(.*?)s$
# Now, we start with an s followed by some smaller number of characters
# followed by an s at the end of the string 

# Summary:
# Regular expressions are used in many different languages; not just in R 
# Regular expressions are composed of literals and metacharacters that represent sets or classes 
# of characters/words 
# Text processing via regular expressions is a very powerful way to extract data from 
# "unfriendly" sources (not all data comes as a CSV file) such as free text format 
# or in a structure data format, where we are trying to just get an access to a particular phrase 
# Used with the functions grep, grepl, sub, gsub that involve searching for text strings 
 
# Working with dates
d1 <- date()
d1# "Fri Jan 29 18:26:44 2021"
class(d1)# "character"

d2 <- Sys.Date()
d2# "2021-01-29"
class(d2)# "Date"

# to reformat d2 into a differnt format
# %a = abbreviated weekday, %A = unabbreviated weekday 
# %b = abbreviated month, %B = unabbreviated month
# %d = day as number (0-31)
# %y = 2 digit year
# %Y = 4 digit year 
format(d2, "%a %b %d")# "Fri Jan 29"
weekdays(d2)# "Friday"
months(d2)# "January"
# Converting to Julian
julian(d2)# number of days that have occured since the origin "1970-01-01"

# To turn character vectors into dates 
x <- c("1jan1960", "2jan1960", "31mar1960", "30jul1960"); z <- as.Date(x, "%d%b%Y")
x
z
z[1] - z[2]
as.numeric(z[1] - z[2])

# lubridate 
# The lubridate package makes it easy to work with dates 
library(lubridate); ymd("20140108")# convert the number to a date regardless of what the format is
mdy("08/04/2013")
dmy("03-04-2013")

ymd_hms("2011-08-03 10:15:03")# will turn this into a time, including hours, minutes and seconds
# if this was in a different order you could use ymd_smh
ymd_hms("2011-08-03 10:15:03", tz = "Pacific/Auckland")# to set time zone
?Sys.timezone# to learn more about how to set time zones  

x <- dmy(c("1jan2013", "2jan2013", "3jan2013", "30jul2013"))
wday(x[1])# to wday is from the lubridate package amd similar to weekdays from R, with different syntax
# this will give you the number of week day, so here 3
wday(x[1], label = T)# and if you want to know the abbreviation of weekday use label = T
# here Tue

# Ultimately you want your dates and times as class "Date" or the classes "POSIXct", "POSIXlt".
# For more information ?POSIXlt 

# Some API's with R interfaces
# 
install.packages("twitteR")
install.packages("Rfacebook")
install.packages("RgoogleMaps")

# Swirl _ Dates abd Times with lubridate
# In this lesson, we'll explore the lubridate R package, by Garrett Grolemund and Hadley Wickham.
# According to the package authors, "lubridate has a consistent, memorable syntax, that makes working
# with dates fun instead of frustrating." If you've ever worked with dates in R, that statement probably
# has your attention.
library(lubridate)
# lubridate contains many useful functions. We'll only be covering the basics here. Type help(package =
# lubridate) to bring up an overview of the package, including the package DESCRIPTION, a list of
# available functions, and a link to the official package vignette.
help(package = lubridate)

# The today() function returns today's date. Give it a try, storing the result in a new variable called
# this_day.
this_day <- today()
this_day
# There are three components to this date. In order, they are year, month, and day. We can extract any of
# these components using the year(), month(), or day() function, respectively. Try any of those on
# this_day now.
year(this_day)
# We can also get the day of the week from this_day using the wday() function. It will be represented as
# a number, such that 1 = Sunday, 2 = Monday, 3 = Tuesday, etc. Give it a shot.
wday(this_day)# 7
# Now try the same thing again, except this time add a second argument, label = TRUE, to display the
# *name* of the weekday (represented as an ordered factor).
wday(this_day, label = TRUE)
# In addition to handling dates, lubridate is great for working with date and time combinations, referred
# to as date-times. The now() function returns the date-time representing this exact moment in time. Give
# it a try and store the result in a variable called this_moment.
this_moment <- now()
this_moment# "2021-01-30 11:42:08 EST"
# Just like with dates, we can extract the year, month, day, or day of week. However, we can also use
# hour(), minute(), and second() to extract specific time information. Try any of these three new
# functions now to extract one piece of time information from this_moment.
hour(this_moment)# 11
# today() and now() provide neatly formatted date-time information. When working with dates and times 'in
# the wild', this won't always (and perhaps rarely will) be the case.

# Fortunately, lubridate offers a variety of functions for parsing date-times. These functions take the
# form of ymd(), dmy(), hms(), ymd_hms(), etc., where each letter in the name of the function stands for
# the location of years (y), months (m), days (d), hours (h), minutes (m), and/or seconds (s) in the
# date-time being read in.

# To see how these functions work, try ymd("1989-05-17"). You must surround the date with quotes. Store
# the result in a variable called my_date.
my_date <- ymd("1989-05-17")
my_date
# It looks almost the same, except for the addition of a time zone, which we'll discuss later in the lesson.
# Below the surface, there's another important change that takes place when lubridate parses a date. Type
# class(my_date) to see what that is.
class(my_date)#"Date
# So ymd() took a character string as input and returned an object of class POSIXct. It's not necessary that
# you understand what POSIXct is, but just know that it is one way that R stores date-time information
# internally.

# "1989-05-17" is a fairly standard format, but lubridate is 'smart' enough to figure out many different
# date-time formats. Use ymd() to parse "1989 May 17". Don't forget to put quotes around the date!
ymd("1989 May 17")

# Despite being formatted differently, the last two dates had the year first, then the month, then the day.
# Hence, we used ymd() to parse them. What do you think the appropriate function is for parsing "March 12,
# 1975"? Give it a try.
mdy("March 12, 1975")

# We can even throw something funky at it and lubridate will often know the right thing to do. Parse 25081985,
# which is supposed to represent the 25th day of August 1985. Note that we are actually parsing a numeric value
# here -- not a character string -- so leave off the quotes.
dmy(25081985)

# But be careful, it's not magic. Try ymd("192012") to see what happens when we give it something more
# ambiguous. Surround the number with quotes again, just to be consistent with the way most dates are
# represented (as character strings).
ymd("192012")# NA
# You got a warning message because it was unclear what date you wanted. When in doubt, it's best to be more
# explicit. Repeat the same command, but add two dashes OR two forward slashes to "192012" so that it's clear
# we want January 2, 1920.
ymd("1920-1-2")

# In addition to dates, we can parse date-times. I've created a date-time object called dt1. Take a look at it
# now.
dt1# "2014-08-23 17:23:02"
ymd_hms(dt1)
# hat if we have a time, but no date? Use the appropriate lubridate function to parse "03:22:14" (hh:mm:ss).
hms("03:22:14")
# lubridate is also capable of handling vectors of dates, which is particularly helpful when you need to parse an entire column of data. I've
# created a vector of dates called dt2. View its contents now.
dt2
ymd(dt2)

# The update() function allows us to update one or more components of a date-time. For example, let's say the current time is 08:34:55 (hh:mm:ss).
# Update this_moment to the new time using the following command:
update(this_moment, hours = 8, minutes = 34, seconds = 55)# 2021-01-30 08:34:55 EST"
# It's important to recognize that the previous command does not alter this_moment unless we reassign the result to this_moment. To see this, print
# the contents of this_moment.
this_moment
this_moment <- update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment

# Now, pretend you are in New York City and you are planning to visit a friend in Hong Kong. You seem to have misplaced your itinerary, but you know
# that your flight departs New York at 17:34 (5:34pm) the day after tomorrow. You also know that your flight is scheduled to arrive in Hong Kong
# exactly 15 hours and 50 minutes after departure.

# Let's reconstruct your itinerary from what you can remember, starting with the full date and time of your departure. We will approach this by
# finding the current date in New York, adding 2 full days, then setting the time to 17:34.

# Let's reconstruct your itinerary from what you can remember, starting with the full date and time of your departure. We will approach this by
# finding the current date in New York, adding 2 full days, then setting the time to 17:34.

# To find the current date in New York, we'll use the now() function again. This time, however, we'll specify the time zone that we want:
# "America/New_York". Store the result in a variable called nyc. Check out ?now if you need help.
nyc <- now(tzone = "America/New_York")
# For a complete list of valid time zones for use with lubridate, check out the following Wikipedia page:
# http://en.wikipedia.org/wiki/List_of_tz_database_time_zones
nyc# "2021-01-31 07:39:00 EST"

# Your flight is the day after tomorrow (in New York time), so we want to add two days to nyc. One nice aspect of lubridate is that it allows you
# to use arithmetic operators on dates and times. In this case, we'd like to add two days to nyc, so we can use the following expression: nyc +
# days(2). Give it a try, storing the result in a variable called depart.
depart <- nyc + days(2)
depart

# So now depart contains the date of the day after tomorrow. Use update() to add the correct hours (17) and minutes (34) to depart. Reassign the
# result to depart.
depart <- update(depart, hours = 17, minutes = 34)
depart

# Your friend wants to know what time she should pick you up from the airport in Hong Kong. Now that we have the exact date and time of your
# departure from New York, we can figure out the exact time of your arrival in Hong Kong.
arrive <- depart + hours(15) + minutes(50)

# The arrive variable contains the time that it will be in New York when you arrive in Hong Kong. What we really want to know is what time it will
# be in Hong Kong when you arrive, so that your friend knows when to meet you.
# The with_tz() function returns a date-time as it would appear in another time zone. Use ?with_tz to check out the documentation.
?with_tz

# Use with_tz() to convert arrive to the "Asia/Hong_Kong" time zone. Reassign the result to arrive, so that it will get the new value.
arrive <- with_tz(arrive, tzone = "Asia/Hong_Kong")
arrive

# Use the appropriate lubridate function to parse "June 17, 2008", just like you did near the beginning of this lesson. This time, however, you
# should specify an extra argument, tz = "Singapore". Store the result in a variable called last_time.
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time

# Pull up the documentation for interval(), which we'll use to explore how much time has passed between arrive and last_time.
?interval
how_long <- interval(last_time, arrive)

# Now use as.period(how_long) to see how long it's been.
as.period(how_long)# "12y 7m 17d 22H 24M 0S"
# This is where things get a little tricky. Because of things like leap years, leap seconds, and daylight savings time, the length of
# any given minute, day, month, week, or year is relative to when it occurs. In contrast, the length of a second is always the same,
# regardless of when it occurs.
# To address these complexities, the authors of lubridate introduce four classes of time related objects: instants, intervals,
# durations, and periods. These topics are beyond the scope of this lesson, but you can find a complete discussion in the 2011
# Journal of Statistical Software paper titled 'Dates and Times Made Easy with lubridate'.

furl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(furl, destfile = "./data/Idahohousing.csv", method = "curl")
Idahohousing <- read.csv("./data/Idahohousing.csv")
head(Idahohousing)
# Apply strsplit() to split all the names of the data frame on the characters "wgtp". 
# What is the value of the 123 element of the resulting list?
Ih <- names(Idahohousing)
class(Ih)#character
splitNames <- strsplit(Ih, "wgtp")
splitNames[[123]]# ""   "15"

# Load the Gross Domestic Product data for the 190 ranked countries in this data set:
fiurl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(fiurl, destfile = "./data/GDP.csv", method = "curl")
GDP <- read.csv("./data/GDP.csv")
head(GDP)
# Remove the commas from the GDP numbers in millions of dollars and average them. What is the average?
# Original data sources:
# http://data.worldbank.org/data-catalog/GDP-ranking-table
View(GDP)
CleanGDP <- GDP[(5:194), ]
View(CleanGDP)
CleanGDP[,"X.3"]
gsub(",", "", CleanGDP[,"X.3"], )# to substitute , with nothing in the names of reviews 
as.numeric(gsub(",", "", CleanGDP[,"X.3"], ))
ave(as.numeric(gsub(",", "", CleanGDP[,"X.3"], )))#377652.4

# In the data set from Question 2 what is a regular expression that would allow you to count the number of countries whose name begins with "United"? 
# Assume that the variable with the country names in it is named countryNames. How many countries begin with United?
countryNames <- CleanGDP[,"X.2"]
grep("^United",countryNames)#3

# Question 4
# Load the Gross Domestic Product data for the 190 ranked countries in this data set:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv 
# Load the educational data from this data set:
# https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv
# Match the data based on the country shortcode. Of the countries for which the end of the fiscal year is available, how many end in June?
SpecialNotes <- mergedgdpedu$Special.Notes
SpecialNotes
grep("June",SpecialNotes)
length(grep("June",SpecialNotes))# 16

# You can use the quantmod (http://www.quantmod.com/) package to get historical stock prices for publicly traded companies on the NASDAQ and NYSE. 
# Use the following code to download data on Amazon's stock price and get the times the data was sampled.
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
head(amzn)
View(amzn)
head(sampleTimes)
# How many values were collected in 2012? How many values were collected on Mondays in 2012?
sampleTimes
length(grep("^2012",sampleTimes))# 250

amzn2012 <- sampleTimes[grep("^2012",sampleTimes)] 
sum(wday(amzn2012, label = TRUE) == "Mon")

# Peer-graded Assignment: Getting and Cleaning Data Course Project
# The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. 
# The goal is to prepare tidy data that can be used for later analysis. 
# You will be required to submit: 1) a tidy data set as described below, 
# 2) a link to a Github repository with your script for performing the analysis, 
# and 3) a code book that describes the variables, the data, and any transformations or work that you performed 
# to clean up the data called CodeBook.md. 
# You should also include a README.md in the repo with your scripts. 
# This repo explains how all of the scripts work and how they are connected.

# One of the most exciting areas in all of data science right now is wearable computing - see for example this article . 
# Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. 
# The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. 
# A full description is available at the site where the data was obtained:
# http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones 
# Here are the data for the project:
# "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"  

# You should create one R script called run_analysis.R that does the following. 
# 1. Merges the training and the test sets to create one data set.
# 2. Extracts only the measurements on the mean and standard deviation for each measurement. 
# 3. Uses descriptive activity names to name the activities in the data set
# 4. Appropriately labels the data set with descriptive variable names. 
# 5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

# 1. Merge the training and the test sets to create one data set:
# test data
testsubject <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/test/subject_test.txt")# 2947    1
testX <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/test/X_test.txt")# 2947  561
testy <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/test/y_test.txt")# 2947    1

# train data
trainsubject <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/train/subject_train.txt")# 7352    1
trainX <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/train/X_train.txt")# 7352  561
trainy <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/train/y_train.txt")# 7352    1

# merge data
sdata <- rbind(testsubject, trainsubject)# 10299     1
xdata <- rbind(testX, trainX)# 10299   561
ydata <- rbind(testy, trainy)# 10299     1

#3. load feature & activity info
# feature info
feature <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/features.txt")
class(feature[,2])#factor
as.character(feature[,2])

# activity labels
activitylabel <- read.table("/Users/maryamghaedi/data/UCI\ HAR\ Dataset/activity_labels.txt")
class(activitylabel[,2])#factor
as.character(activitylabel[,2])

# 2. Extracts only the measurements on the mean and standard deviation for each measurement. 
# extract feature cols & names named 'mean, std'
selectedCols <- grep("mean|std", as.character(feature[,2]))
selectedCols
selectedColNames <- feature[selectedCols, 2]
selectedColNames
selectedColNames <- sub("-mean", "Mean", selectedColNames)
selectedColNames <- sub("-std", "Std", selectedColNames)
selectedColNames
selectedColNames <- gsub("[-()]", "", selectedColNames)
selectedColNames

#4. extract data by cols & using descriptive name
xdata <- xdata[selectedCols]
allData <- cbind(sdata, ydata, xdata)
colnames(allData) <- c("Subject", "Activity", selectedColNames)

allData$Activity
allData$Activity <- factor(allData$Activity, levels = activitylabel[,1], labels = activitylabel[,2])
allData$Subject
allData$Subject <- as.factor(allData$Subject)

#5. generate tidy data set with the average of each variable for each activity and each subject.
library(reshape2)
head(allData)
meltedData <- melt(allData, id = c("Subject", "Activity"))
meltedData
tidyData <- dcast(meltedData, Subject + Activity ~ variable, mean)
dim(tidyData)
write.table(tidyData, "./tidy_dataset.txt", row.names = FALSE, quote = FALSE)


